{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoVunR-YfYao"
      },
      "source": [
        "# In case, the file import data from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ6JGzB4fpzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82f70583-18c5-4af1-ba57-08da9ea5aa89"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYoemWhpfrUm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a927c0-b0af-41b3-d76b-1c862180653b"
      },
      "source": [
        "%cd drive/MyDrive/Colab\\ Notebooks\n",
        "# !ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OgetZCcfr_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34cb7fe1-aed8-4f8d-d631-6dcfc862344c"
      },
      "source": [
        "# Install libraries\n",
        "!pip install mne\n",
        "!pip install pyriemann\n",
        "!pip install MOABB\n",
        "!pip install  scipy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.7.3)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.7/dist-packages (from mne) (1.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mne) (3.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mne) (4.64.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mne) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.21.6)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mne) (2.11.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mne) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mne) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mne) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mne) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyriemann in /usr/local/lib/python3.7/dist-packages (0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyriemann) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyriemann) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyriemann) (1.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyriemann) (1.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pyriemann) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pyriemann) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pyriemann) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->pyriemann) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyriemann) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: MOABB in /usr/local/lib/python3.7/dist-packages (0.4.6)\n",
            "Requirement already satisfied: pooch<2.0,>=1.6 in /usr/local/lib/python3.7/dist-packages (from MOABB) (1.6.0)\n",
            "Requirement already satisfied: h5py<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from MOABB) (3.1.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.62 in /usr/local/lib/python3.7/dist-packages (from MOABB) (4.64.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.0 in /usr/local/lib/python3.7/dist-packages (from MOABB) (1.21.6)\n",
            "Requirement already satisfied: mne>=0.19 in /usr/local/lib/python3.7/dist-packages (from MOABB) (1.1.1)\n",
            "Requirement already satisfied: pandas<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from MOABB) (1.3.5)\n",
            "Requirement already satisfied: PyYAML<6.0,>=5.0 in /usr/local/lib/python3.7/dist-packages (from MOABB) (5.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.15.1 in /usr/local/lib/python3.7/dist-packages (from MOABB) (2.23.0)\n",
            "Requirement already satisfied: scipy<2.0,>=1.5 in /usr/local/lib/python3.7/dist-packages (from MOABB) (1.7.3)\n",
            "Requirement already satisfied: seaborn>=0.9 in /usr/local/lib/python3.7/dist-packages (from MOABB) (0.11.2)\n",
            "Requirement already satisfied: scikit-learn<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from MOABB) (1.0.2)\n",
            "Requirement already satisfied: pyriemann>=0.2.6 in /usr/local/lib/python3.7/dist-packages (from MOABB) (0.3)\n",
            "Requirement already satisfied: matplotlib<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from MOABB) (3.2.2)\n",
            "Requirement already satisfied: coverage<6.0,>=5.5 in /usr/local/lib/python3.7/dist-packages (from MOABB) (5.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py<4.0,>=3.0->MOABB) (1.5.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0,>=3.0->MOABB) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0,>=3.0->MOABB) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0,>=3.0->MOABB) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0,>=3.0->MOABB) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<4.0,>=3.0->MOABB) (4.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mne>=0.19->MOABB) (2.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mne>=0.19->MOABB) (21.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mne>=0.19->MOABB) (4.4.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0,>=1.0->MOABB) (2022.2.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch<2.0,>=1.6->MOABB) (1.4.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyriemann>=0.2.6->MOABB) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib<4.0,>=3.0->MOABB) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.15.1->MOABB) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.15.1->MOABB) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.15.1->MOABB) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.15.1->MOABB) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<2.0,>=1.0->MOABB) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mne>=0.19->MOABB) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxUttm0PfwLI"
      },
      "source": [
        "# References\n",
        "- EEGANet: Removal of Ocular Artifact from the EEG Signal Using Generative Adversarial Networks\n",
        "    - https://ieeexplore.ieee.org/document/9627782\n",
        "    - https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/GANs/SRGAN\n",
        "    - https://github.com/eriklindernoren/PyTorch-GAN/tree/master/implementations/srgan\n",
        "- Datasets\n",
        "    - http://moabb.neurotechx.com/docs/generated/moabb.datasets.BNCI2014004.html#moabb.datasets.BNCI2014004\n",
        "- Notes\n",
        "    - "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tciqSn9gPh7"
      },
      "source": [
        "## Import libraries and read files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Raw1IZobgPes"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import mne\n",
        "from mne import find_events, Epochs, pick_types, read_evokeds\n",
        "from mne.preprocessing import ICA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pywt\n",
        "import scipy\n",
        "from mne.preprocessing import (ICA, create_eog_epochs, create_ecg_epochs,\n",
        "                               corrmap)\n",
        "from sklearn.neighbors import KDTree\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import warnings\n",
        "mne.set_log_level(\"CRITICAL\")\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "nvCPCbQXvf1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_event_ids = 20\n",
        "selected_data_len = 2000\n",
        "\n",
        "# 60 events/ 1 run/ 3 sessions/ 1 subject\n",
        "batch_size = 10\n",
        "n_subjs = 20\n",
        "events_per_run = 20\n",
        "runs_per_session = 1\n",
        "sessions_for_train = 2\n",
        "sessions_for_eval = 1\n",
        "events_per_subj_for_train = int(events_per_run * runs_per_session * sessions_for_train)\n",
        "events_per_subj_for_eval = int(events_per_run * runs_per_session * sessions_for_eval)\n",
        "train_split = int(events_per_subj_for_train * 0.8)\n",
        "\n",
        "list_epoch_types = [\"All epochs\", \"Bad\", \"Good\"]"
      ],
      "metadata": {
        "id": "4HrUJup7S7UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EOG_ref = \"EEGANet\"\n",
        "ECG_ref = None\n",
        "\n",
        "# LOAD EEG DATA\n",
        "train_good_epochs = np.load(f'Datasets/Shin2017A/gb_npy_files/train_good_epochs_{EOG_ref}-{ECG_ref}.npy')\n",
        "train_bad_epochs = np.load(f'Datasets/Shin2017A/gb_npy_files/train_bad_epochs_{EOG_ref}-{ECG_ref}.npy')\n",
        "eval_good_epochs = np.load(f'Datasets/Shin2017A/gb_npy_files/eval_good_epochs_{EOG_ref}-{ECG_ref}.npy')\n",
        "eval_bad_epochs = np.load(f'Datasets/Shin2017A/gb_npy_files/eval_bad_epochs_{EOG_ref}-{ECG_ref}.npy')\n",
        "\n",
        "# LOAD EVENT\n",
        "train_good_events = np.load(f'Datasets/Shin2017A/gb_npy_files/train_good_events_{EOG_ref}-{ECG_ref}.npy')\n",
        "train_bad_events = np.load(f'Datasets/Shin2017A/gb_npy_files/train_bad_events_{EOG_ref}-{ECG_ref}.npy')\n",
        "eval_good_events = np.load(f'Datasets/Shin2017A/gb_npy_files/eval_good_events_{EOG_ref}-{ECG_ref}.npy')\n",
        "eval_bad_events = np.load(f'Datasets/Shin2017A/gb_npy_files/eval_bad_events_{EOG_ref}-{ECG_ref}.npy')"
      ],
      "metadata": {
        "id": "6j36TNFxpM7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_good_epochs.shape, train_bad_epochs.shape,\n",
        "      eval_good_epochs.shape, eval_bad_epochs.shape,\n",
        "      train_good_events.shape, train_bad_events.shape,\n",
        "      eval_good_events.shape, eval_bad_events.shape)"
      ],
      "metadata": {
        "id": "EJHARR7tpYLp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe965eb-e72d-484c-9ee8-7cf1e08c3af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(651, 30, 2000) (149, 30, 2000) (324, 30, 2000) (76, 30, 2000) (651, 3) (149, 3) (324, 3) (76, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.concatenate((train_good_epochs, train_bad_epochs), axis=0).shape)\n",
        "print(np.concatenate((train_good_events, train_bad_events), axis=0).shape)"
      ],
      "metadata": {
        "id": "CVd-GB1UpYjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e91f4577-68a8-4738-ee8e-039ef7ed1e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(800, 30, 2000)\n",
            "(800, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification\n",
        "- 01 = CSP + SVM\n",
        "- 02 = CSP + LDA\n",
        "- 03 = EEGNet\n",
        "    - Note of EEGNET's problems\n",
        "        - FC Layer\n",
        "            x = x.reshape(-1, 4*2*7)\n",
        "        - Label must start with 0\n",
        "    - Data format:\n",
        "        Datatype - float32 (both X and Y) <br>\n",
        "        X.shape - (#samples, 1, #timepoints,  #channels) <br>\n",
        "        Y.shape - (#samples)"
      ],
      "metadata": {
        "id": "tuX126rF0xHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "YMHpJ9AJIkvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_for_EEGNET(input_epochs):\n",
        "    # X.shape - (#epochs, 1, #timepoints, #channels)\n",
        "    output_epochs = []\n",
        "    for e_epoch in input_epochs:\n",
        "        e_epoch = e_epoch.transpose()\n",
        "        e_epoch = np.expand_dims(e_epoch,axis=0)\n",
        "        # print(e_epoch.shape)\n",
        "        output_epochs.append(e_epoch)\n",
        "        # break\n",
        "    return np.array(output_epochs).astype('float32')"
      ],
      "metadata": {
        "id": "_ydzwhmf20TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        # Modify self.T, self.fc1 \n",
        "        super(EEGNet, self).__init__()\n",
        "        self.T = 2000 # #samples per task\n",
        "        \n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Conv2d(1, 16, (1, 30), padding = 0)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
        "        \n",
        "        # Layer 2\n",
        "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
        "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
        "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
        "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
        "        \n",
        "        # Layer 3\n",
        "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
        "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
        "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
        "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
        "        \n",
        "        # FC Layer\n",
        "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
        "        # I have 120 timepoints. \n",
        "        self.fc1 = nn.Linear(4*2*125, 1)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        x = F.elu(self.conv1(x))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = F.dropout(x, 0.25)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        \n",
        "        # Layer 2\n",
        "        x = self.padding1(x)\n",
        "        x = F.elu(self.conv2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = F.dropout(x, 0.25)\n",
        "        x = self.pooling2(x)\n",
        "        \n",
        "        # Layer 3\n",
        "        x = self.padding2(x)\n",
        "        x = F.elu(self.conv3(x))\n",
        "        x = self.batchnorm3(x)\n",
        "        x = F.dropout(x, 0.25)\n",
        "        x = self.pooling3(x)\n",
        "        \n",
        "        # FC Layer\n",
        "        x = x.reshape(-1, 4*2*125)\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "net = EEGNet().cuda(0)\n",
        "print(net.forward(Variable(torch.Tensor(np.random.rand(1, 1, 2000, 30)).cuda(0))))\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(net.parameters())"
      ],
      "metadata": {
        "id": "df-fHsWHIksX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db998b4c-4b40-4ff4-ce7c-a5c7a1b8d885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2315]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, X, Y, params = [\"acc\"]):\n",
        "    results = []\n",
        "    batch_size = 100\n",
        "    \n",
        "    predicted = []\n",
        "    \n",
        "    for i in range(int(len(X)/batch_size)):\n",
        "        s = i*batch_size\n",
        "        e = i*batch_size+batch_size\n",
        "        \n",
        "        inputs = Variable(torch.from_numpy(X[s:e]).cuda(0))\n",
        "        pred = model(inputs)\n",
        "        \n",
        "        predicted.append(pred.data.cpu().numpy())\n",
        "        \n",
        "        \n",
        "    inputs = Variable(torch.from_numpy(X).cuda(0))\n",
        "    predicted = model(inputs)\n",
        "    \n",
        "    predicted = predicted.data.cpu().numpy()\n",
        "    \n",
        "    for param in params:\n",
        "        if param == 'acc':\n",
        "            results.append(accuracy_score(Y, np.round(predicted)))\n",
        "        if param == \"auc\":\n",
        "            results.append(roc_auc_score(Y, predicted))\n",
        "        if param == \"recall\":\n",
        "            results.append(recall_score(Y, np.round(predicted)))\n",
        "        if param == \"precision\":\n",
        "            results.append(precision_score(Y, np.round(predicted)))\n",
        "        if param == \"fmeasure\":\n",
        "            precision = precision_score(Y, np.round(predicted))\n",
        "            recall = recall_score(Y, np.round(predicted))\n",
        "            results.append(2*precision*recall/ (precision+recall))\n",
        "    return results"
      ],
      "metadata": {
        "id": "BtfKU_3xN9OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "DrlCt2eUQ-Z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx_e_subj in range(3):\n",
        "    train_split = int(events_per_subj_for_train * 0.8)\n",
        "    # Assign data and label for Training, Validation, Evaluation\n",
        "\n",
        "    start_idx_train = int(idx_e_subj * events_per_subj_for_train)\n",
        "    end_idx_train = start_idx_train + int(events_per_subj_for_train * 0.8)\n",
        "    start_idx_val = end_idx_train\n",
        "    end_idx_val = int((idx_e_subj+1) * events_per_subj_for_train)\n",
        "    start_idx_eval = int((idx_e_subj)*events_per_subj_for_eval)\n",
        "    end_idx_eval  = int((idx_e_subj+1)*events_per_subj_for_eval)\n",
        "\n",
        "    print(f\"start_idx_train: {start_idx_train}\")\n",
        "    print(f\"end_idx_train: {end_idx_train}\")\n",
        "    print(f\"start_idx_val: {start_idx_val}\")\n",
        "    print(f\"end_idx_val: {end_idx_val}\")\n",
        "    print(f\"start_idx_eval: {start_idx_eval}\")\n",
        "    print(f\"end_idx_eval: {end_idx_eval}\")\n",
        "    print('='*100)\n",
        "\n",
        "    # break"
      ],
      "metadata": {
        "id": "Gvt5M-MiYjFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b7d5f56-315a-4860-f24e-85ebf62a4601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_idx_train: 0\n",
            "end_idx_train: 32\n",
            "start_idx_val: 32\n",
            "end_idx_val: 40\n",
            "start_idx_eval: 0\n",
            "end_idx_eval: 20\n",
            "====================================================================================================\n",
            "start_idx_train: 40\n",
            "end_idx_train: 72\n",
            "start_idx_val: 72\n",
            "end_idx_val: 80\n",
            "start_idx_eval: 20\n",
            "end_idx_eval: 40\n",
            "====================================================================================================\n",
            "start_idx_train: 80\n",
            "end_idx_train: 112\n",
            "start_idx_val: 112\n",
            "end_idx_val: 120\n",
            "start_idx_eval: 40\n",
            "end_idx_eval: 60\n",
            "====================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_good_epochs.shape, train_bad_epochs.shape,\n",
        "      eval_good_epochs.shape, eval_bad_epochs.shape,\n",
        "      train_good_events.shape, train_bad_events.shape,\n",
        "      eval_good_events.shape, eval_bad_events.shape)"
      ],
      "metadata": {
        "id": "vXvOa-Doq3Of",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc677e8b-f6af-438f-d897-93f7757d1876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(651, 30, 2000) (149, 30, 2000) (324, 30, 2000) (76, 30, 2000) (651, 3) (149, 3) (324, 3) (76, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_epoch_types"
      ],
      "metadata": {
        "id": "m0qVqsvcrcdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e9b717-bfb0-41d9-a648-97f92e3238f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['All epochs', 'Bad', 'Good']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_test_types = []\n",
        "\n",
        "for idx_e_type, e_type in enumerate(list_epoch_types):\n",
        "    print(e_type)\n",
        "\n",
        "    if e_type == 'All epochs':\n",
        "\n",
        "        X = np.concatenate((train_good_epochs, train_bad_epochs), axis=0)\n",
        "        y = np.concatenate((train_good_events, train_bad_events), axis=0)[:, -1]\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,\n",
        "                                                          shuffle=True, \n",
        "                                                          stratify=y,\n",
        "                                                          random_state=42)\n",
        "        \n",
        "        X_test = np.concatenate((eval_good_epochs, eval_bad_epochs), axis=0)\n",
        "        y_test = np.concatenate((eval_good_events, eval_bad_events), axis=0)[:, -1]\n",
        "\n",
        "    elif e_type == 'Bad':\n",
        "\n",
        "        X = train_bad_epochs\n",
        "        y = train_bad_events[:, -1]\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,\n",
        "                                                          shuffle=True, \n",
        "                                                          stratify=y,\n",
        "                                                          random_state=42)\n",
        "        \n",
        "        X_test = eval_bad_epochs\n",
        "        y_test = eval_bad_events[:, -1]\n",
        "\n",
        "    elif e_type == 'Good':\n",
        "\n",
        "        X = train_good_epochs\n",
        "        y = train_good_events[:, -1]\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,\n",
        "                                                          shuffle=True, \n",
        "                                                          stratify=y,\n",
        "                                                          random_state=42)\n",
        "        \n",
        "        X_test = eval_good_epochs\n",
        "        y_test = eval_good_events[:, -1]\n",
        "\n",
        "    print(X.shape, y.shape)\n",
        "    print(X[0,0,:5])\n",
        "    print(y[:5])\n",
        "\n",
        "    # Modify the input data and its label\n",
        "    X_train = data_for_EEGNET(X_train)\n",
        "    X_val = data_for_EEGNET(X_val)\n",
        "    X_test = data_for_EEGNET(X_test)\n",
        "\n",
        "    y_train = y_train - 1 # Start with 0\n",
        "    y_val = y_val - 1 # Start with 0\n",
        "    y_test = y_test - 1 # Start with 0\n",
        "\n",
        "    print(f\"start_idx_train: {start_idx_train}\")\n",
        "    print(f\"end_idx_train: {end_idx_train}\")\n",
        "    print(f\"start_idx_val: {start_idx_val}\")\n",
        "    print(f\"end_idx_val: {end_idx_val}\")\n",
        "    print(f\"start_idx_eval: {start_idx_eval}\")\n",
        "    print(f\"end_idx_eval: {end_idx_eval}\")\n",
        "\n",
        "    print(y_train[:10])\n",
        "    print(y_val[:10])\n",
        "    print(y_test[:10])\n",
        "\n",
        "    print('-'*100)\n",
        "    \n",
        "    # Assign a model and its hyperparameters\n",
        "    # del net\n",
        "    net = EEGNet().cuda(0)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "    list_training_loss = []\n",
        "    list_train_performance = []\n",
        "    list_valid_performance = []\n",
        "    list_test_performance = []\n",
        "\n",
        "    for epoch in range(100):  # loop over the dataset multiple times\n",
        "        print(\"\\nEpoch \", epoch)\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for i in range(int((len(X_train)/batch_size-1))):\n",
        "            s = i*batch_size\n",
        "            e = i*batch_size+batch_size\n",
        "            \n",
        "            inputs = torch.from_numpy(X_train[s:e])\n",
        "            labels = torch.FloatTensor(np.array([y_train[s:e]]).T*1.0)\n",
        "            \n",
        "            # wrap them in Variable\n",
        "            inputs, labels = Variable(inputs.cuda(0)), Variable(labels.cuda(0))\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            \n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            \n",
        "        # Validation accuracy\n",
        "        net.eval()\n",
        "        params = [\"acc\", \"auc\", \"fmeasure\"]\n",
        "        print(params)\n",
        "\n",
        "        e_train_performance = evaluate(net, X_train, y_train, params)\n",
        "        e_valid_performance = evaluate(net, X_val, y_val, params)\n",
        "        e_test_performance = evaluate(net, X_test, y_test, params)\n",
        "\n",
        "        print(\"Training Loss \", running_loss)\n",
        "        print(\"Train - \", e_train_performance)\n",
        "        print(\"Validation - \", e_valid_performance)\n",
        "        print(\"Test - \", e_test_performance)\n",
        "\n",
        "        list_training_loss.append(running_loss)\n",
        "        list_train_performance.append(e_train_performance)\n",
        "        list_valid_performance.append(e_valid_performance)\n",
        "        list_test_performance.append(e_test_performance)\n",
        "\n",
        "    list_training_loss = np.array(list_training_loss)\n",
        "    list_train_performance = np.array(list_train_performance)\n",
        "    list_valid_performance = np.array(list_valid_performance)\n",
        "    list_test_performance = np.array(list_test_performance)\n",
        "\n",
        "    best_acc = np.argmax(list_test_performance[:,0])\n",
        "    print(f\"Epoch with best accuracy is {best_acc}\")\n",
        "    best_test_performance = list_test_performance[best_acc,:]\n",
        "    best_test_types.append(best_test_performance)\n",
        "    print(f\"'='*50 + 'END OF {e_type} epoch type + '='*50\")\n"
      ],
      "metadata": {
        "id": "fVwgBNo7z-7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace9e92b-b3f4-42bf-e564-0338bcb05339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All epochs\n",
            "(800, 30, 2000) (800,)\n",
            "[-0.08427826 -0.08716451 -0.0831288  -0.07596581 -0.07719874]\n",
            "[1 2 1 2 1]\n",
            "start_idx_train: 80\n",
            "end_idx_train: 112\n",
            "start_idx_val: 112\n",
            "end_idx_val: 120\n",
            "start_idx_eval: 40\n",
            "end_idx_eval: 60\n",
            "[0 1 1 0 0 1 1 1 1 1]\n",
            "[1 0 0 1 1 0 1 0 0 1]\n",
            "[0 1 1 0 1 0 0 1 0 1]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Epoch  0\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  54.78417718410492\n",
            "Train -  [0.5296875, 0.604013671875, 0.3080459770114943]\n",
            "Validation -  [0.51875, 0.48578125, 0.3063063063063063]\n",
            "Test -  [0.505, 0.532775, 0.2773722627737226]\n",
            "\n",
            "Epoch  1\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  44.53570529818535\n",
            "Train -  [0.584375, 0.663134765625, 0.38425925925925924]\n",
            "Validation -  [0.55, 0.53453125, 0.33333333333333337]\n",
            "Test -  [0.4975, 0.5201250000000001, 0.28975265017667845]\n",
            "\n",
            "Epoch  2\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  42.98382091522217\n",
            "Train -  [0.590625, 0.7132128906249999, 0.35148514851485146]\n",
            "Validation -  [0.53125, 0.5278125, 0.2424242424242424]\n",
            "Test -  [0.4925, 0.50545, 0.26181818181818184]\n",
            "\n",
            "Epoch  3\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  39.62404555082321\n",
            "Train -  [0.6421875, 0.761982421875, 0.4944812362030904]\n",
            "Validation -  [0.55, 0.5326562499999999, 0.3454545454545454]\n",
            "Test -  [0.5225, 0.5437500000000001, 0.37377049180327865]\n",
            "\n",
            "Epoch  4\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  38.35301852226257\n",
            "Train -  [0.6765625, 0.79341796875, 0.5784114052953157]\n",
            "Validation -  [0.6, 0.5674999999999999, 0.4666666666666667]\n",
            "Test -  [0.5675, 0.575675, 0.43278688524590164]\n",
            "\n",
            "Epoch  5\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  36.65558335185051\n",
            "Train -  [0.7140625, 0.8226074218749999, 0.6376237623762376]\n",
            "Validation -  [0.61875, 0.5824999999999999, 0.5040650406504065]\n",
            "Test -  [0.5825, 0.6058, 0.46984126984126984]\n",
            "\n",
            "Epoch  6\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  35.4533312022686\n",
            "Train -  [0.7296875, 0.830322265625, 0.6882882882882883]\n",
            "Validation -  [0.6125, 0.6190625000000001, 0.5373134328358209]\n",
            "Test -  [0.5875, 0.610075, 0.5074626865671642]\n",
            "\n",
            "Epoch  7\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  33.115834072232246\n",
            "Train -  [0.7484375, 0.85568359375, 0.710951526032316]\n",
            "Validation -  [0.61875, 0.6112499999999998, 0.5271317829457365]\n",
            "Test -  [0.5575, 0.6044499999999999, 0.4809384164222874]\n",
            "\n",
            "Epoch  8\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  32.428335428237915\n",
            "Train -  [0.7859375, 0.87591796875, 0.7633851468048359]\n",
            "Validation -  [0.5625, 0.6337499999999999, 0.5070422535211268]\n",
            "Test -  [0.5825, 0.5961749999999999, 0.5373961218836565]\n",
            "\n",
            "Epoch  9\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  30.501471683382988\n",
            "Train -  [0.7765625, 0.8898046875, 0.7395264116575593]\n",
            "Validation -  [0.6, 0.63234375, 0.4920634920634921]\n",
            "Test -  [0.605, 0.633425, 0.5240963855421686]\n",
            "\n",
            "Epoch  10\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  29.401653215289116\n",
            "Train -  [0.775, 0.8969628906249999, 0.7313432835820897]\n",
            "Validation -  [0.6, 0.639375, 0.48387096774193555]\n",
            "Test -  [0.61, 0.62375, 0.524390243902439]\n",
            "\n",
            "Epoch  11\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  27.684891059994698\n",
            "Train -  [0.80625, 0.9049511718749998, 0.7816901408450704]\n",
            "Validation -  [0.55625, 0.64890625, 0.4580152671755725]\n",
            "Test -  [0.59, 0.619875, 0.5393258426966293]\n",
            "\n",
            "Epoch  12\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  26.492903351783752\n",
            "Train -  [0.8015625, 0.9054785156249999, 0.7858347386172007]\n",
            "Validation -  [0.60625, 0.6325000000000001, 0.5333333333333333]\n",
            "Test -  [0.5725, 0.6091249999999999, 0.5155807365439095]\n",
            "\n",
            "Epoch  13\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  25.410520374774933\n",
            "Train -  [0.846875, 0.933447265625, 0.8333333333333334]\n",
            "Validation -  [0.65, 0.6823437499999999, 0.6056338028169015]\n",
            "Test -  [0.6, 0.6324, 0.5652173913043478]\n",
            "\n",
            "Epoch  14\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  24.288154371082783\n",
            "Train -  [0.8546875, 0.9344042968749999, 0.8447412353923205]\n",
            "Validation -  [0.6375, 0.6784374999999999, 0.5972222222222222]\n",
            "Test -  [0.615, 0.6327499999999999, 0.5649717514124294]\n",
            "\n",
            "Epoch  15\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  23.420251935720444\n",
            "Train -  [0.846875, 0.9351660156249999, 0.8388157894736843]\n",
            "Validation -  [0.55625, 0.63890625, 0.5170068027210885]\n",
            "Test -  [0.6, 0.6452499999999999, 0.581151832460733]\n",
            "\n",
            "Epoch  16\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  21.524830751121044\n",
            "Train -  [0.8515625, 0.9479296875, 0.8324514991181658]\n",
            "Validation -  [0.61875, 0.6504687499999999, 0.5196850393700788]\n",
            "Test -  [0.58, 0.6105250000000001, 0.5227272727272727]\n",
            "\n",
            "Epoch  17\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  21.11658288538456\n",
            "Train -  [0.8546875, 0.9520214843749999, 0.8436974789915966]\n",
            "Validation -  [0.6375, 0.66140625, 0.5797101449275363]\n",
            "Test -  [0.615, 0.630825, 0.5698324022346368]\n",
            "\n",
            "Epoch  18\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  19.906567707657814\n",
            "Train -  [0.878125, 0.947060546875, 0.8704318936877077]\n",
            "Validation -  [0.625, 0.6628125, 0.6000000000000001]\n",
            "Test -  [0.63, 0.64505, 0.612565445026178]\n",
            "\n",
            "Epoch  19\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  18.265354298055172\n",
            "Train -  [0.834375, 0.954814453125, 0.8044280442804428]\n",
            "Validation -  [0.64375, 0.666875, 0.5365853658536585]\n",
            "Test -  [0.6075, 0.6203, 0.5285285285285286]\n",
            "\n",
            "Epoch  20\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  17.932853624224663\n",
            "Train -  [0.853125, 0.954580078125, 0.833922261484099]\n",
            "Validation -  [0.63125, 0.65453125, 0.5629629629629629]\n",
            "Test -  [0.6275, 0.627325, 0.5802816901408451]\n",
            "\n",
            "Epoch  21\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  17.103950664401054\n",
            "Train -  [0.8796875, 0.9600390625, 0.8705882352941177]\n",
            "Validation -  [0.65, 0.6832812500000001, 0.6266666666666667]\n",
            "Test -  [0.62, 0.643725, 0.5978835978835979]\n",
            "\n",
            "Epoch  22\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  15.447225321084261\n",
            "Train -  [0.8671875, 0.9704003906249999, 0.8495575221238937]\n",
            "Validation -  [0.6, 0.6709375, 0.5223880597014926]\n",
            "Test -  [0.6175, 0.622425, 0.5539358600583091]\n",
            "\n",
            "Epoch  23\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  15.647792294621468\n",
            "Train -  [0.865625, 0.968466796875, 0.8485915492957746]\n",
            "Validation -  [0.6375, 0.6646875000000001, 0.5322580645161289]\n",
            "Test -  [0.595, 0.6118999999999999, 0.5423728813559322]\n",
            "\n",
            "Epoch  24\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  14.774538848549128\n",
            "Train -  [0.90625, 0.96876953125, 0.90625]\n",
            "Validation -  [0.63125, 0.685625, 0.6040268456375839]\n",
            "Test -  [0.615, 0.6422749999999999, 0.6206896551724137]\n",
            "\n",
            "Epoch  25\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  14.285403233021498\n",
            "Train -  [0.878125, 0.969853515625, 0.8650519031141868]\n",
            "Validation -  [0.65, 0.67796875, 0.5942028985507246]\n",
            "Test -  [0.62, 0.6267499999999999, 0.5681818181818182]\n",
            "\n",
            "Epoch  26\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  13.207154851406813\n",
            "Train -  [0.9109375, 0.9773242187500001, 0.9042016806722688]\n",
            "Validation -  [0.6, 0.67109375, 0.5492957746478874]\n",
            "Test -  [0.595, 0.630875, 0.5597826086956522]\n",
            "\n",
            "Epoch  27\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  13.584644939750433\n",
            "Train -  [0.8671875, 0.97185546875, 0.8511383537653241]\n",
            "Validation -  [0.63125, 0.6840624999999999, 0.556390977443609]\n",
            "Test -  [0.5975, 0.61025, 0.5386819484240688]\n",
            "\n",
            "Epoch  28\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  12.332416742108762\n",
            "Train -  [0.9046875, 0.978740234375, 0.8960817717206133]\n",
            "Validation -  [0.6625, 0.70078125, 0.619718309859155]\n",
            "Test -  [0.63, 0.63195, 0.588888888888889]\n",
            "\n",
            "Epoch  29\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  14.18058817088604\n",
            "Train -  [0.89375, 0.96775390625, 0.8885245901639344]\n",
            "Validation -  [0.6375, 0.7028125000000001, 0.6133333333333333]\n",
            "Test -  [0.5875, 0.622225, 0.5576407506702412]\n",
            "\n",
            "Epoch  30\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  12.562954742461443\n",
            "Train -  [0.875, 0.97154296875, 0.8591549295774648]\n",
            "Validation -  [0.5875, 0.67421875, 0.4761904761904762]\n",
            "Test -  [0.6025, 0.6202000000000001, 0.5364431486880467]\n",
            "\n",
            "Epoch  31\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  13.006776629015803\n",
            "Train -  [0.840625, 0.9740820312499999, 0.8104089219330856]\n",
            "Validation -  [0.63125, 0.7146874999999999, 0.5123966942148761]\n",
            "Test -  [0.6, 0.6066, 0.5031055900621119]\n",
            "\n",
            "Epoch  32\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  12.525429148226976\n",
            "Train -  [0.8765625, 0.9805078125, 0.861646234676007]\n",
            "Validation -  [0.6, 0.68546875, 0.5076923076923077]\n",
            "Test -  [0.59, 0.6274249999999999, 0.5232558139534884]\n",
            "\n",
            "Epoch  33\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  10.10176164098084\n",
            "Train -  [0.884375, 0.9811328124999998, 0.8701754385964913]\n",
            "Validation -  [0.5875, 0.6871875000000001, 0.4590163934426229]\n",
            "Test -  [0.585, 0.604425, 0.5059523809523809]\n",
            "\n",
            "Epoch  34\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  11.152648568153381\n",
            "Train -  [0.8921875, 0.979619140625, 0.8816466552315608]\n",
            "Validation -  [0.625, 0.6610937499999999, 0.53125]\n",
            "Test -  [0.595, 0.6267250000000001, 0.5149700598802395]\n",
            "\n",
            "Epoch  35\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  9.539850804954767\n",
            "Train -  [0.890625, 0.9839648437499999, 0.8793103448275861]\n",
            "Validation -  [0.61875, 0.66015625, 0.5271317829457365]\n",
            "Test -  [0.58, 0.619525, 0.5]\n",
            "\n",
            "Epoch  36\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  9.649430647492409\n",
            "Train -  [0.909375, 0.9817871093749999, 0.9006849315068494]\n",
            "Validation -  [0.6625, 0.6971875, 0.6029411764705882]\n",
            "Test -  [0.6225, 0.61725, 0.5673352435530087]\n",
            "\n",
            "Epoch  37\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  11.059183090459555\n",
            "Train -  [0.9015625, 0.9779003906249999, 0.8941176470588236]\n",
            "Validation -  [0.60625, 0.6662499999999999, 0.5401459854014599]\n",
            "Test -  [0.6025, 0.613925, 0.5444126074498566]\n",
            "\n",
            "Epoch  38\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  9.416737380437553\n",
            "Train -  [0.921875, 0.98447265625, 0.9155405405405406]\n",
            "Validation -  [0.61875, 0.6560937499999999, 0.5413533834586466]\n",
            "Test -  [0.5825, 0.609575, 0.5214899713467049]\n",
            "\n",
            "Epoch  39\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  9.835763370618224\n",
            "Train -  [0.9390625, 0.9843359375, 0.937799043062201]\n",
            "Validation -  [0.6375, 0.67328125, 0.6329113924050633]\n",
            "Test -  [0.5875, 0.636025, 0.5905707196029777]\n",
            "\n",
            "Epoch  40\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  9.540430191904306\n",
            "Train -  [0.93125, 0.98240234375, 0.9266666666666667]\n",
            "Validation -  [0.6125, 0.65109375, 0.5694444444444444]\n",
            "Test -  [0.585, 0.587825, 0.5363128491620112]\n",
            "\n",
            "Epoch  41\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  9.65788843575865\n",
            "Train -  [0.915625, 0.981015625, 0.91]\n",
            "Validation -  [0.6625, 0.6915625, 0.619718309859155]\n",
            "Test -  [0.5975, 0.6224749999999999, 0.5660377358490566]\n",
            "\n",
            "Epoch  42\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  10.454698078334332\n",
            "Train -  [0.9296875, 0.9810449218749999, 0.9282296650717704]\n",
            "Validation -  [0.6125, 0.6403125, 0.6075949367088608]\n",
            "Test -  [0.6, 0.6277, 0.5876288659793814]\n",
            "\n",
            "Epoch  43\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  9.43737587891519\n",
            "Train -  [0.940625, 0.985380859375, 0.939297124600639]\n",
            "Validation -  [0.5875, 0.65140625, 0.5714285714285715]\n",
            "Test -  [0.6, 0.60335, 0.5833333333333334]\n",
            "\n",
            "Epoch  44\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.237915471196175\n",
            "Train -  [0.9421875, 0.988095703125, 0.9392446633825945]\n",
            "Validation -  [0.68125, 0.6951562499999999, 0.6433566433566433]\n",
            "Test -  [0.61, 0.6352249999999999, 0.5851063829787234]\n",
            "\n",
            "Epoch  45\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.765500588342547\n",
            "Train -  [0.953125, 0.9890039062500001, 0.9528301886792452]\n",
            "Validation -  [0.64375, 0.719375, 0.6459627329192547]\n",
            "Test -  [0.575, 0.614475, 0.5893719806763283]\n",
            "\n",
            "Epoch  46\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  10.224052826175466\n",
            "Train -  [0.9234375, 0.9841796875, 0.9192751235584843]\n",
            "Validation -  [0.6375, 0.66953125, 0.6027397260273972]\n",
            "Test -  [0.5875, 0.6191, 0.5504087193460491]\n",
            "\n",
            "Epoch  47\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.735449685715139\n",
            "Train -  [0.925, 0.9871972656250001, 0.9200000000000002]\n",
            "Validation -  [0.61875, 0.6812499999999999, 0.5673758865248227]\n",
            "Test -  [0.595, 0.6110875, 0.5449438202247192]\n",
            "\n",
            "Epoch  48\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.45206887461245\n",
            "Train -  [0.9578125, 0.9905078125000001, 0.957613814756672]\n",
            "Validation -  [0.61875, 0.67515625, 0.5906040268456376]\n",
            "Test -  [0.5925, 0.6196249999999999, 0.5975308641975308]\n",
            "\n",
            "Epoch  49\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  7.742929412517697\n",
            "Train -  [0.9421875, 0.9871777343749999, 0.9429892141756548]\n",
            "Validation -  [0.6375, 0.6784375, 0.6375]\n",
            "Test -  [0.5975, 0.6296625, 0.6082725060827251]\n",
            "\n",
            "Epoch  50\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  7.088972038589418\n",
            "Train -  [0.9328125, 0.985654296875, 0.9347496206373294]\n",
            "Validation -  [0.61875, 0.6724218750000001, 0.6303030303030304]\n",
            "Test -  [0.5475, 0.6019125000000001, 0.578088578088578]\n",
            "\n",
            "Epoch  51\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.729502353118733\n",
            "Train -  [0.94375, 0.985927734375, 0.9430379746835443]\n",
            "Validation -  [0.625, 0.6842187499999999, 0.6153846153846154]\n",
            "Test -  [0.615, 0.6432374999999999, 0.6130653266331657]\n",
            "\n",
            "Epoch  52\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.332756113260984\n",
            "Train -  [0.946875, 0.9852246093750001, 0.9460317460317461]\n",
            "Validation -  [0.63125, 0.68, 0.6289308176100629]\n",
            "Test -  [0.5875, 0.592475, 0.5736434108527132]\n",
            "\n",
            "Epoch  53\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.291878564748913\n",
            "Train -  [0.940625, 0.98517578125, 0.941717791411043]\n",
            "Validation -  [0.60625, 0.66515625, 0.6440677966101694]\n",
            "Test -  [0.5775, 0.617275, 0.6078886310904873]\n",
            "\n",
            "Epoch  54\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.566807199269533\n",
            "Train -  [0.9453125, 0.9860546874999999, 0.946236559139785]\n",
            "Validation -  [0.5875, 0.63765625, 0.5925925925925926]\n",
            "Test -  [0.5875, 0.6291499999999999, 0.6099290780141844]\n",
            "\n",
            "Epoch  55\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.808090135222301\n",
            "Train -  [0.9578125, 0.9931054687499999, 0.957074721780604]\n",
            "Validation -  [0.65625, 0.68640625, 0.6258503401360543]\n",
            "Test -  [0.585, 0.622925, 0.5721649484536082]\n",
            "\n",
            "Epoch  56\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  7.849663839908317\n",
            "Train -  [0.9375, 0.987880859375, 0.9344262295081966]\n",
            "Validation -  [0.65, 0.6954687499999999, 0.6164383561643836]\n",
            "Test -  [0.5975, 0.6166, 0.5613079019073569]\n",
            "\n",
            "Epoch  57\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.295600394718349\n",
            "Train -  [0.9046875, 0.9832714843749999, 0.8953687821612352]\n",
            "Validation -  [0.625, 0.6910937500000001, 0.5454545454545455]\n",
            "Test -  [0.6025, 0.6199749999999999, 0.5444126074498566]\n",
            "\n",
            "Epoch  58\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.947120159631595\n",
            "Train -  [0.8703125, 0.9837011718750001, 0.8509874326750448]\n",
            "Validation -  [0.5875, 0.6576562499999999, 0.46774193548387094]\n",
            "Test -  [0.6175, 0.6417875, 0.5349544072948328]\n",
            "\n",
            "Epoch  59\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  10.923879876499996\n",
            "Train -  [0.896875, 0.9842480468750001, 0.886986301369863]\n",
            "Validation -  [0.63125, 0.6825, 0.549618320610687]\n",
            "Test -  [0.59, 0.6135, 0.5367231638418078]\n",
            "\n",
            "Epoch  60\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  7.948516195639968\n",
            "Train -  [0.934375, 0.988974609375, 0.9299999999999999]\n",
            "Validation -  [0.6375, 0.69875, 0.5857142857142856]\n",
            "Test -  [0.58, 0.601, 0.5333333333333332]\n",
            "\n",
            "Epoch  61\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.582191706635058\n",
            "Train -  [0.9484375, 0.9945507812499998, 0.9459901800327332]\n",
            "Validation -  [0.63125, 0.69734375, 0.5815602836879432]\n",
            "Test -  [0.5775, 0.6049500000000001, 0.554089709762533]\n",
            "\n",
            "Epoch  62\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.372969700023532\n",
            "Train -  [0.95, 0.988857421875, 0.9487179487179489]\n",
            "Validation -  [0.625, 0.6603125, 0.5833333333333334]\n",
            "Test -  [0.6075, 0.6266625, 0.5922077922077922]\n",
            "\n",
            "Epoch  63\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.874073557322845\n",
            "Train -  [0.953125, 0.98921875, 0.9517684887459807]\n",
            "Validation -  [0.66875, 0.6981250000000001, 0.6394557823129252]\n",
            "Test -  [0.59, 0.6243000000000001, 0.5684210526315789]\n",
            "\n",
            "Epoch  64\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.55677672685124\n",
            "Train -  [0.94375, 0.989658203125, 0.9433962264150944]\n",
            "Validation -  [0.6125, 0.67109375, 0.6025641025641025]\n",
            "Test -  [0.5775, 0.6073125, 0.5806451612903226]\n",
            "\n",
            "Epoch  65\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.55779498280026\n",
            "Train -  [0.9421875, 0.9872070312500001, 0.943683409436834]\n",
            "Validation -  [0.6125, 0.67421875, 0.6265060240963854]\n",
            "Test -  [0.59, 0.62125, 0.6255707762557078]\n",
            "\n",
            "Epoch  66\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.280068541294895\n",
            "Train -  [0.95, 0.9913671874999999, 0.948220064724919]\n",
            "Validation -  [0.64375, 0.6795312499999999, 0.6225165562913908]\n",
            "Test -  [0.5825, 0.58795, 0.5498652291105122]\n",
            "\n",
            "Epoch  67\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.817261387361214\n",
            "Train -  [0.9296875, 0.986728515625, 0.9253731343283581]\n",
            "Validation -  [0.59375, 0.66828125, 0.5390070921985816]\n",
            "Test -  [0.62, 0.6348875, 0.5869565217391305]\n",
            "\n",
            "Epoch  68\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.433221637154929\n",
            "Train -  [0.940625, 0.9898828124999999, 0.9377049180327869]\n",
            "Validation -  [0.6375, 0.658125, 0.5972222222222222]\n",
            "Test -  [0.62, 0.6140125, 0.5935828877005349]\n",
            "\n",
            "Epoch  69\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  7.288873841054738\n",
            "Train -  [0.9421875, 0.9902148437499999, 0.9388429752066115]\n",
            "Validation -  [0.65, 0.694375, 0.6111111111111112]\n",
            "Test -  [0.5975, 0.6137375, 0.5589041095890411]\n",
            "\n",
            "Epoch  70\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.426324927480891\n",
            "Train -  [0.9375, 0.9878125, 0.935064935064935]\n",
            "Validation -  [0.61875, 0.65984375, 0.5960264900662251]\n",
            "Test -  [0.6, 0.6216875000000001, 0.5876288659793814]\n",
            "\n",
            "Epoch  71\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.422990688821301\n",
            "Train -  [0.9421875, 0.9870507812499999, 0.9422776911076443]\n",
            "Validation -  [0.59375, 0.6698437500000001, 0.5859872611464968]\n",
            "Test -  [0.5925, 0.6206, 0.5955334987593052]\n",
            "\n",
            "Epoch  72\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.149128145072609\n",
            "Train -  [0.9515625, 0.992421875, 0.9497568881685575]\n",
            "Validation -  [0.6, 0.6551562499999999, 0.5428571428571427]\n",
            "Test -  [0.59, 0.61745, 0.5684210526315789]\n",
            "\n",
            "Epoch  73\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.547572813695297\n",
            "Train -  [0.9453125, 0.984609375, 0.9448818897637795]\n",
            "Validation -  [0.63125, 0.6678124999999999, 0.6143790849673203]\n",
            "Test -  [0.5725, 0.6070249999999999, 0.585956416464891]\n",
            "\n",
            "Epoch  74\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  7.8676363420672715\n",
            "Train -  [0.9390625, 0.9833496093749999, 0.937799043062201]\n",
            "Validation -  [0.625, 0.66421875, 0.6103896103896105]\n",
            "Test -  [0.5825, 0.6069625000000001, 0.5772151898734177]\n",
            "\n",
            "Epoch  75\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  7.2271249134792015\n",
            "Train -  [0.9546875, 0.9897167968750001, 0.9541864139020536]\n",
            "Validation -  [0.6125, 0.663515625, 0.6352941176470589]\n",
            "Test -  [0.615, 0.6321625, 0.6262135922330097]\n",
            "\n",
            "Epoch  76\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  7.895759501028806\n",
            "Train -  [0.95, 0.988818359375, 0.9510703363914372]\n",
            "Validation -  [0.61875, 0.6728125, 0.6473988439306358]\n",
            "Test -  [0.5875, 0.6144999999999999, 0.6153846153846154]\n",
            "\n",
            "Epoch  77\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.690770326415077\n",
            "Train -  [0.94375, 0.992705078125, 0.940983606557377]\n",
            "Validation -  [0.6125, 0.65734375, 0.581081081081081]\n",
            "Test -  [0.5825, 0.600025, 0.5593667546174143]\n",
            "\n",
            "Epoch  78\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.696143020293675\n",
            "Train -  [0.9546875, 0.9904296874999999, 0.9536]\n",
            "Validation -  [0.5875, 0.6365624999999999, 0.5657894736842105]\n",
            "Test -  [0.5775, 0.6090125, 0.5699745547073791]\n",
            "\n",
            "Epoch  79\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.704893856338458\n",
            "Train -  [0.95625, 0.992783203125, 0.9548387096774195]\n",
            "Validation -  [0.675, 0.686640625, 0.6578947368421053]\n",
            "Test -  [0.58, 0.6121875, 0.5757575757575758]\n",
            "\n",
            "Epoch  80\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.286227005068213\n",
            "Train -  [0.9640625, 0.9932421875, 0.9638932496075353]\n",
            "Validation -  [0.63125, 0.6721875, 0.6143790849673203]\n",
            "Test -  [0.5525, 0.5947125000000001, 0.5536159600997507]\n",
            "\n",
            "Epoch  81\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.358416589493572\n",
            "Train -  [0.9671875, 0.992939453125, 0.9664]\n",
            "Validation -  [0.625, 0.6425000000000001, 0.6103896103896105]\n",
            "Test -  [0.5975, 0.6255375000000001, 0.5861182519280206]\n",
            "\n",
            "Epoch  82\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.500115672708489\n",
            "Train -  [0.9578125, 0.990556640625, 0.9568]\n",
            "Validation -  [0.58125, 0.642265625, 0.567741935483871]\n",
            "Test -  [0.57, 0.5831875, 0.5544041450777202]\n",
            "\n",
            "Epoch  83\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.295587389729917\n",
            "Train -  [0.9671875, 0.9929296875, 0.9664]\n",
            "Validation -  [0.6375, 0.6631250000000001, 0.6233766233766234]\n",
            "Test -  [0.5775, 0.6077125, 0.5587467362924281]\n",
            "\n",
            "Epoch  84\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.4463454249780625\n",
            "Train -  [0.9546875, 0.989013671875, 0.9544740973312401]\n",
            "Validation -  [0.59375, 0.605078125, 0.6198830409356725]\n",
            "Test -  [0.5675, 0.6027250000000001, 0.5790754257907542]\n",
            "\n",
            "Epoch  85\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.135491490247659\n",
            "Train -  [0.9578125, 0.9900390625, 0.9578783151326054]\n",
            "Validation -  [0.6375, 0.63359375, 0.6419753086419754]\n",
            "Test -  [0.5725, 0.6009625, 0.5879518072289156]\n",
            "\n",
            "Epoch  86\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  3.353881956776604\n",
            "Train -  [0.9703125, 0.9936718749999999, 0.9703588143525741]\n",
            "Validation -  [0.64375, 0.671796875, 0.6586826347305389]\n",
            "Test -  [0.5725, 0.6007125, 0.583941605839416]\n",
            "\n",
            "Epoch  87\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.723661617841572\n",
            "Train -  [0.9671875, 0.9937695312499999, 0.9671361502347419]\n",
            "Validation -  [0.625, 0.674765625, 0.6296296296296297]\n",
            "Test -  [0.5875, 0.6100875, 0.5864661654135338]\n",
            "\n",
            "Epoch  88\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  3.6427470905473456\n",
            "Train -  [0.971875, 0.9935351562499999, 0.9722222222222221]\n",
            "Validation -  [0.625, 0.65359375, 0.6341463414634146]\n",
            "Test -  [0.5625, 0.6071624999999999, 0.580335731414868]\n",
            "\n",
            "Epoch  89\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  3.9351535411442455\n",
            "Train -  [0.971875, 0.9942871093749999, 0.9717868338557993]\n",
            "Validation -  [0.61875, 0.663125, 0.6390532544378699]\n",
            "Test -  [0.57, 0.6169, 0.5924170616113744]\n",
            "\n",
            "Epoch  90\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.976289464975707\n",
            "Train -  [0.965625, 0.9897265625, 0.9661538461538461]\n",
            "Validation -  [0.6125, 0.6382031249999999, 0.6172839506172839]\n",
            "Test -  [0.595, 0.6347875000000001, 0.6105769230769229]\n",
            "\n",
            "Epoch  91\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.806855226750486\n",
            "Train -  [0.94375, 0.9890332031249999, 0.9415584415584415]\n",
            "Validation -  [0.6375, 0.64625, 0.6081081081081082]\n",
            "Test -  [0.6025, 0.624625, 0.5826771653543307]\n",
            "\n",
            "Epoch  92\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.940303901908919\n",
            "Train -  [0.9640625, 0.9926269531249999, 0.9638932496075353]\n",
            "Validation -  [0.61875, 0.6784375, 0.6347305389221558]\n",
            "Test -  [0.615, 0.6245499999999999, 0.6188118811881188]\n",
            "\n",
            "Epoch  93\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.6447509622666985\n",
            "Train -  [0.971875, 0.99513671875, 0.9714285714285715]\n",
            "Validation -  [0.59375, 0.6505468750000001, 0.5859872611464968]\n",
            "Test -  [0.565, 0.5885875, 0.565]\n",
            "\n",
            "Epoch  94\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.6860436967690475\n",
            "Train -  [0.959375, 0.9924999999999999, 0.9587301587301588]\n",
            "Validation -  [0.6, 0.62484375, 0.5897435897435896]\n",
            "Test -  [0.5875, 0.6142375, 0.5843828715365238]\n",
            "\n",
            "Epoch  95\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.029306391545106\n",
            "Train -  [0.9609375, 0.9924609375, 0.959742351046699]\n",
            "Validation -  [0.6125, 0.625625, 0.6075949367088608]\n",
            "Test -  [0.5875, 0.594, 0.5669291338582677]\n",
            "\n",
            "Epoch  96\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.228933050588239\n",
            "Train -  [0.9578125, 0.9922363281250001, 0.957345971563981]\n",
            "Validation -  [0.575, 0.6268750000000001, 0.575]\n",
            "Test -  [0.585, 0.6156625, 0.5951219512195122]\n",
            "\n",
            "Epoch  97\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.384705327509437\n",
            "Train -  [0.9640625, 0.992431640625, 0.9645608628659477]\n",
            "Validation -  [0.575, 0.6361718749999999, 0.6]\n",
            "Test -  [0.5625, 0.5924125, 0.601366742596811]\n",
            "\n",
            "Epoch  98\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.737572553509381\n",
            "Train -  [0.9453125, 0.9889648437499999, 0.943089430894309]\n",
            "Validation -  [0.575, 0.6460937499999999, 0.5405405405405405]\n",
            "Test -  [0.5925, 0.60225, 0.5606469002695418]\n",
            "\n",
            "Epoch  99\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.240034188493155\n",
            "Train -  [0.9484375, 0.99001953125, 0.9471999999999999]\n",
            "Validation -  [0.64375, 0.67046875, 0.6369426751592356]\n",
            "Test -  [0.59, 0.600775, 0.5879396984924624]\n",
            "Epoch with best accuracy is 18\n",
            "'='*50 + 'END OF All epochs epoch type + '='*50\n",
            "Bad\n",
            "(149, 30, 2000) (149,)\n",
            "[-0.07345069 -0.08111654 -0.08487285 -0.08220156 -0.07745113]\n",
            "[2 1 2 1 1]\n",
            "start_idx_train: 80\n",
            "end_idx_train: 112\n",
            "start_idx_val: 112\n",
            "end_idx_val: 120\n",
            "start_idx_eval: 40\n",
            "end_idx_eval: 60\n",
            "[0 0 0 0 0 0 1 0 0 0]\n",
            "[1 0 1 0 1 0 1 0 0 1]\n",
            "[1 1 0 0 0 1 0 1 0 0]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Epoch  0\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.790345311164856\n",
            "Train -  [0.5210084033613446, 0.5195025438100622, 0.6627218934911242]\n",
            "Validation -  [0.5333333333333333, 0.48888888888888893, 0.6666666666666667]\n",
            "Test -  [0.4605263157894737, 0.5768115942028985, 0.5940594059405941]\n",
            "\n",
            "Epoch  1\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.813149034976959\n",
            "Train -  [0.5630252100840336, 0.5457885811192764, 0.6000000000000001]\n",
            "Validation -  [0.43333333333333335, 0.3288888888888889, 0.5405405405405405]\n",
            "Test -  [0.4605263157894737, 0.4985507246376812, 0.5176470588235295]\n",
            "\n",
            "Epoch  2\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.405030220746994\n",
            "Train -  [0.5882352941176471, 0.6916336913510457, 0.6620689655172414]\n",
            "Validation -  [0.5, 0.52, 0.5714285714285715]\n",
            "Test -  [0.39473684210526316, 0.5014492753623189, 0.46511627906976744]\n",
            "\n",
            "Epoch  3\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.07703772187233\n",
            "Train -  [0.6638655462184874, 0.7351611079706049, 0.6551724137931035]\n",
            "Validation -  [0.5, 0.4311111111111111, 0.4]\n",
            "Test -  [0.5526315789473685, 0.5449275362318841, 0.43333333333333335]\n",
            "\n",
            "Epoch  4\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.829183250665665\n",
            "Train -  [0.680672268907563, 0.776144714527982, 0.7076923076923076]\n",
            "Validation -  [0.5, 0.56, 0.4444444444444445]\n",
            "Test -  [0.47368421052631576, 0.5231884057971015, 0.42857142857142855]\n",
            "\n",
            "Epoch  5\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.421971559524536\n",
            "Train -  [0.7142857142857143, 0.8157150932730356, 0.7068965517241379]\n",
            "Validation -  [0.5333333333333333, 0.5155555555555557, 0.5333333333333333]\n",
            "Test -  [0.5921052631578947, 0.5753623188405796, 0.5633802816901409]\n",
            "\n",
            "Epoch  6\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.318914085626602\n",
            "Train -  [0.7563025210084033, 0.857263990955342, 0.7289719626168225]\n",
            "Validation -  [0.5, 0.5377777777777777, 0.4]\n",
            "Test -  [0.5131578947368421, 0.5144927536231885, 0.39344262295081966]\n",
            "\n",
            "Epoch  7\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.00739198923111\n",
            "Train -  [0.7478991596638656, 0.8609383832673827, 0.7457627118644067]\n",
            "Validation -  [0.5, 0.5111111111111111, 0.4827586206896552]\n",
            "Test -  [0.5394736842105263, 0.5775362318840581, 0.4927536231884058]\n",
            "\n",
            "Epoch  8\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  3.6622557044029236\n",
            "Train -  [0.773109243697479, 0.8595251554550594, 0.7610619469026548]\n",
            "Validation -  [0.43333333333333335, 0.4533333333333333, 0.32]\n",
            "Test -  [0.6052631578947368, 0.5876811594202899, 0.5]\n",
            "\n",
            "Epoch  9\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  3.5338441729545593\n",
            "Train -  [0.8151260504201681, 0.8954211418880723, 0.8166666666666667]\n",
            "Validation -  [0.43333333333333335, 0.5155555555555555, 0.32]\n",
            "Test -  [0.5, 0.5376811594202898, 0.4722222222222222]\n",
            "\n",
            "Epoch  10\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  3.3450343906879425\n",
            "Train -  [0.7647058823529411, 0.8815715093273034, 0.7666666666666666]\n",
            "Validation -  [0.5666666666666667, 0.568888888888889, 0.48]\n",
            "Test -  [0.5789473684210527, 0.5637681159420289, 0.5]\n",
            "\n",
            "Epoch  11\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  3.164457231760025\n",
            "Train -  [0.7899159663865546, 0.899095534200113, 0.7899159663865546]\n",
            "Validation -  [0.5333333333333333, 0.47111111111111115, 0.4166666666666667]\n",
            "Test -  [0.5526315789473685, 0.5659420289855072, 0.5277777777777778]\n",
            "\n",
            "Epoch  12\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  3.1181830912828445\n",
            "Train -  [0.8235294117647058, 0.9143583945732052, 0.8264462809917356]\n",
            "Validation -  [0.6, 0.56, 0.5384615384615385]\n",
            "Test -  [0.5263157894736842, 0.5413043478260869, 0.5]\n",
            "\n",
            "Epoch  13\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  3.0306412130594254\n",
            "Train -  [0.8151260504201681, 0.919728660260034, 0.8166666666666667]\n",
            "Validation -  [0.5666666666666667, 0.5777777777777778, 0.48]\n",
            "Test -  [0.631578947368421, 0.6007246376811594, 0.611111111111111]\n",
            "\n",
            "Epoch  14\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  2.702067509293556\n",
            "Train -  [0.8403361344537815, 0.9287733182589033, 0.8403361344537814]\n",
            "Validation -  [0.5666666666666667, 0.5555555555555556, 0.5185185185185186]\n",
            "Test -  [0.6052631578947368, 0.5702898550724638, 0.5588235294117647]\n",
            "\n",
            "Epoch  15\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  2.5776865780353546\n",
            "Train -  [0.8403361344537815, 0.9389485585076315, 0.8429752066115703]\n",
            "Validation -  [0.5666666666666667, 0.5777777777777777, 0.5185185185185186]\n",
            "Test -  [0.5921052631578947, 0.5768115942028985, 0.5753424657534246]\n",
            "\n",
            "Epoch  16\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  2.516684338450432\n",
            "Train -  [0.8487394957983193, 0.9386659129451668, 0.8474576271186439]\n",
            "Validation -  [0.5333333333333333, 0.5733333333333333, 0.5]\n",
            "Test -  [0.6052631578947368, 0.5739130434782609, 0.5454545454545454]\n",
            "\n",
            "Epoch  17\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  2.3770465701818466\n",
            "Train -  [0.8991596638655462, 0.9465799886941776, 0.9]\n",
            "Validation -  [0.5666666666666667, 0.5644444444444444, 0.5185185185185186]\n",
            "Test -  [0.5526315789473685, 0.5659420289855073, 0.5142857142857143]\n",
            "\n",
            "Epoch  18\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  2.2177703380584717\n",
            "Train -  [0.8907563025210085, 0.9457320520067835, 0.888888888888889]\n",
            "Validation -  [0.5333333333333333, 0.5822222222222222, 0.4166666666666667]\n",
            "Test -  [0.5921052631578947, 0.596376811594203, 0.5507246376811593]\n",
            "\n",
            "Epoch  19\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  2.174230992794037\n",
            "Train -  [0.9243697478991597, 0.9581684567552288, 0.9268292682926829]\n",
            "Validation -  [0.6333333333333333, 0.6177777777777778, 0.5925925925925926]\n",
            "Test -  [0.5789473684210527, 0.5920289855072464, 0.5675675675675677]\n",
            "\n",
            "Epoch  20\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  2.194017767906189\n",
            "Train -  [0.8823529411764706, 0.9556246466930468, 0.8852459016393442]\n",
            "Validation -  [0.5666666666666667, 0.5644444444444445, 0.5185185185185186]\n",
            "Test -  [0.6052631578947368, 0.6217391304347827, 0.5945945945945945]\n",
            "\n",
            "Epoch  21\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  1.8715439289808273\n",
            "Train -  [0.907563025210084, 0.9556246466930469, 0.9090909090909091]\n",
            "Validation -  [0.6, 0.5911111111111111, 0.5]\n",
            "Test -  [0.5394736842105263, 0.5333333333333333, 0.47761194029850745]\n",
            "\n",
            "Epoch  22\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  1.8252693489193916\n",
            "Train -  [0.9243697478991597, 0.9544940644431882, 0.9243697478991596]\n",
            "Validation -  [0.5666666666666667, 0.5822222222222222, 0.5185185185185186]\n",
            "Test -  [0.5657894736842105, 0.591304347826087, 0.547945205479452]\n",
            "\n",
            "Epoch  23\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  1.7864258289337158\n",
            "Train -  [0.907563025210084, 0.9485585076314302, 0.9043478260869565]\n",
            "Validation -  [0.6666666666666666, 0.6311111111111112, 0.5833333333333334]\n",
            "Test -  [0.5657894736842105, 0.6072463768115942, 0.547945205479452]\n",
            "\n",
            "Epoch  24\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  1.7854328081011772\n",
            "Train -  [0.8991596638655462, 0.9561899378179762, 0.9016393442622952]\n",
            "Validation -  [0.6, 0.6488888888888888, 0.5]\n",
            "Test -  [0.5526315789473685, 0.555072463768116, 0.5277777777777778]\n",
            "\n",
            "Epoch  25\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  1.6847108416259289\n",
            "Train -  [0.9159663865546218, 0.964951950254381, 0.9180327868852459]\n",
            "Validation -  [0.6, 0.6311111111111111, 0.5714285714285715]\n",
            "Test -  [0.5921052631578947, 0.596376811594203, 0.5753424657534246]\n",
            "\n",
            "Epoch  26\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  1.5165895149111748\n",
            "Train -  [0.9327731092436975, 0.9643866591294515, 0.9322033898305084]\n",
            "Validation -  [0.7, 0.6177777777777779, 0.64]\n",
            "Test -  [0.5921052631578947, 0.5797101449275363, 0.5753424657534246]\n",
            "\n",
            "Epoch  27\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  1.4478501752018929\n",
            "Train -  [0.9327731092436975, 0.9708875070661391, 0.9322033898305084]\n",
            "Validation -  [0.6, 0.5733333333333333, 0.5]\n",
            "Test -  [0.5921052631578947, 0.5478260869565217, 0.5507246376811593]\n",
            "\n",
            "Epoch  28\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  1.4128399714827538\n",
            "Train -  [0.9243697478991597, 0.9635387224420576, 0.9243697478991596]\n",
            "Validation -  [0.6666666666666666, 0.6355555555555554, 0.6153846153846153]\n",
            "Test -  [0.5657894736842105, 0.596376811594203, 0.5217391304347826]\n",
            "\n",
            "Epoch  29\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  1.246931053698063\n",
            "Train -  [0.9411764705882353, 0.9626907857546636, 0.9411764705882353]\n",
            "Validation -  [0.5333333333333333, 0.6133333333333333, 0.4615384615384615]\n",
            "Test -  [0.5921052631578947, 0.591304347826087, 0.5866666666666667]\n",
            "\n",
            "Epoch  30\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  1.2997274659574032\n",
            "Train -  [0.9243697478991597, 0.9663651780667044, 0.923076923076923]\n",
            "Validation -  [0.6, 0.5822222222222222, 0.5]\n",
            "Test -  [0.5, 0.5557971014492753, 0.4571428571428572]\n",
            "\n",
            "Epoch  31\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  1.2100493088364601\n",
            "Train -  [0.9327731092436975, 0.9680610514414923, 0.9333333333333333]\n",
            "Validation -  [0.6, 0.5955555555555555, 0.5]\n",
            "Test -  [0.6052631578947368, 0.581159420289855, 0.5454545454545454]\n",
            "\n",
            "Epoch  32\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  1.246394369751215\n",
            "Train -  [0.907563025210084, 0.9629734313171283, 0.9075630252100839]\n",
            "Validation -  [0.6, 0.6133333333333333, 0.5]\n",
            "Test -  [0.5526315789473685, 0.5579710144927537, 0.5277777777777778]\n",
            "\n",
            "Epoch  33\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  1.1564659662544727\n",
            "Train -  [0.9159663865546218, 0.9626907857546636, 0.9166666666666666]\n",
            "Validation -  [0.6333333333333333, 0.6177777777777778, 0.56]\n",
            "Test -  [0.5789473684210527, 0.5963768115942029, 0.5428571428571427]\n",
            "\n",
            "Epoch  34\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.9970179051160812\n",
            "Train -  [0.9243697478991597, 0.9697569248162804, 0.9279999999999999]\n",
            "Validation -  [0.5666666666666667, 0.6355555555555554, 0.5185185185185186]\n",
            "Test -  [0.5526315789473685, 0.5949275362318841, 0.575]\n",
            "\n",
            "Epoch  35\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  1.0061350874602795\n",
            "Train -  [0.9159663865546218, 0.9643866591294518, 0.9152542372881356]\n",
            "Validation -  [0.5666666666666667, 0.5733333333333334, 0.48]\n",
            "Test -  [0.5263157894736842, 0.5521739130434783, 0.48571428571428565]\n",
            "\n",
            "Epoch  36\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.9827264361083508\n",
            "Train -  [0.907563025210084, 0.965799886941775, 0.9075630252100839]\n",
            "Validation -  [0.6, 0.6044444444444445, 0.5]\n",
            "Test -  [0.5921052631578947, 0.6065217391304347, 0.523076923076923]\n",
            "\n",
            "Epoch  37\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.8993993438780308\n",
            "Train -  [0.9327731092436975, 0.9737139626907858, 0.9333333333333333]\n",
            "Validation -  [0.6333333333333333, 0.5466666666666666, 0.56]\n",
            "Test -  [0.5263157894736842, 0.586231884057971, 0.5263157894736841]\n",
            "\n",
            "Epoch  38\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.9289168789982796\n",
            "Train -  [0.907563025210084, 0.9618428490672697, 0.9059829059829059]\n",
            "Validation -  [0.6, 0.6222222222222222, 0.5]\n",
            "Test -  [0.5789473684210527, 0.6188405797101451, 0.5675675675675677]\n",
            "\n",
            "Epoch  39\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.8757955022156239\n",
            "Train -  [0.9495798319327731, 0.973431317128321, 0.9491525423728813]\n",
            "Validation -  [0.5333333333333333, 0.5511111111111111, 0.4166666666666667]\n",
            "Test -  [0.5789473684210527, 0.6391304347826088, 0.5897435897435898]\n",
            "\n",
            "Epoch  40\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.7843376100063324\n",
            "Train -  [0.9243697478991597, 0.9680610514414925, 0.923076923076923]\n",
            "Validation -  [0.6333333333333333, 0.5777777777777777, 0.47619047619047616]\n",
            "Test -  [0.5789473684210527, 0.6239130434782608, 0.5555555555555556]\n",
            "\n",
            "Epoch  41\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.7841642387211323\n",
            "Train -  [0.9243697478991597, 0.9754098360655737, 0.9243697478991596]\n",
            "Validation -  [0.6333333333333333, 0.6133333333333334, 0.5217391304347827]\n",
            "Test -  [0.5921052631578947, 0.618840579710145, 0.5866666666666667]\n",
            "\n",
            "Epoch  42\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.7794345673173666\n",
            "Train -  [0.9243697478991597, 0.9672131147540984, 0.923076923076923]\n",
            "Validation -  [0.6, 0.5866666666666667, 0.5]\n",
            "Test -  [0.5263157894736842, 0.5637681159420289, 0.5]\n",
            "\n",
            "Epoch  43\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.734140363521874\n",
            "Train -  [0.9243697478991597, 0.9663651780667043, 0.9243697478991596]\n",
            "Validation -  [0.5666666666666667, 0.5688888888888888, 0.43478260869565216]\n",
            "Test -  [0.631578947368421, 0.6152173913043478, 0.611111111111111]\n",
            "\n",
            "Epoch  44\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.6845956603065133\n",
            "Train -  [0.9243697478991597, 0.9796495195025438, 0.9243697478991596]\n",
            "Validation -  [0.5666666666666667, 0.6044444444444445, 0.48]\n",
            "Test -  [0.6052631578947368, 0.6159420289855073, 0.5833333333333334]\n",
            "\n",
            "Epoch  45\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.6252363650128245\n",
            "Train -  [0.9411764705882353, 0.9680610514414923, 0.9411764705882353]\n",
            "Validation -  [0.5666666666666667, 0.6, 0.48]\n",
            "Test -  [0.5263157894736842, 0.5934782608695652, 0.5499999999999999]\n",
            "\n",
            "Epoch  46\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.5918176779523492\n",
            "Train -  [0.9159663865546218, 0.9691916336913511, 0.9137931034482758]\n",
            "Validation -  [0.5666666666666667, 0.6000000000000001, 0.48]\n",
            "Test -  [0.5, 0.5681159420289855, 0.4571428571428572]\n",
            "\n",
            "Epoch  47\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.5518768830224872\n",
            "Train -  [0.8991596638655462, 0.9641040135669868, 0.8983050847457625]\n",
            "Validation -  [0.6, 0.5555555555555556, 0.5]\n",
            "Test -  [0.6052631578947368, 0.6021739130434782, 0.5833333333333334]\n",
            "\n",
            "Epoch  48\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.5770770306698978\n",
            "Train -  [0.9159663865546218, 0.9660825325042396, 0.9152542372881356]\n",
            "Validation -  [0.6333333333333333, 0.6266666666666667, 0.5217391304347827]\n",
            "Test -  [0.5789473684210527, 0.6014492753623188, 0.5789473684210527]\n",
            "\n",
            "Epoch  49\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.5467676604166627\n",
            "Train -  [0.9159663865546218, 0.9731486715658564, 0.9166666666666666]\n",
            "Validation -  [0.6, 0.6222222222222222, 0.5]\n",
            "Test -  [0.5789473684210527, 0.6195652173913043, 0.5555555555555556]\n",
            "\n",
            "Epoch  50\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.8545104246586561\n",
            "Train -  [0.9327731092436975, 0.9641040135669869, 0.9333333333333333]\n",
            "Validation -  [0.6333333333333333, 0.5955555555555556, 0.5217391304347827]\n",
            "Test -  [0.5263157894736842, 0.631159420289855, 0.5384615384615384]\n",
            "\n",
            "Epoch  51\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.654308401979506\n",
            "Train -  [0.8907563025210085, 0.961560203504805, 0.8907563025210085]\n",
            "Validation -  [0.5666666666666667, 0.6, 0.48]\n",
            "Test -  [0.5394736842105263, 0.6231884057971016, 0.5569620253164558]\n",
            "\n",
            "Epoch  52\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.5544474255293608\n",
            "Train -  [0.9411764705882353, 0.9790842283776144, 0.9401709401709402]\n",
            "Validation -  [0.6333333333333333, 0.5911111111111111, 0.5217391304347827]\n",
            "Test -  [0.618421052631579, 0.6275362318840579, 0.5671641791044775]\n",
            "\n",
            "Epoch  53\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.49389593582600355\n",
            "Train -  [0.9327731092436975, 0.9742792538157151, 0.9333333333333333]\n",
            "Validation -  [0.5333333333333333, 0.5955555555555556, 0.4166666666666667]\n",
            "Test -  [0.5526315789473685, 0.6057971014492753, 0.5405405405405405]\n",
            "\n",
            "Epoch  54\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.5693971095606685\n",
            "Train -  [0.9327731092436975, 0.9635387224420575, 0.9344262295081968]\n",
            "Validation -  [0.5666666666666667, 0.5955555555555556, 0.43478260869565216]\n",
            "Test -  [0.5789473684210527, 0.6173913043478261, 0.5897435897435898]\n",
            "\n",
            "Epoch  55\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.5113077545538545\n",
            "Train -  [0.9159663865546218, 0.9544940644431883, 0.9166666666666666]\n",
            "Validation -  [0.6, 0.6488888888888888, 0.5]\n",
            "Test -  [0.5657894736842105, 0.613768115942029, 0.547945205479452]\n",
            "\n",
            "Epoch  56\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.4506854461506009\n",
            "Train -  [0.907563025210084, 0.9576031656302996, 0.9090909090909091]\n",
            "Validation -  [0.5666666666666667, 0.5466666666666666, 0.48]\n",
            "Test -  [0.4868421052631579, 0.5391304347826088, 0.48]\n",
            "\n",
            "Epoch  57\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.4098236858844757\n",
            "Train -  [0.9159663865546218, 0.9550593555681176, 0.9180327868852459]\n",
            "Validation -  [0.5666666666666667, 0.6133333333333333, 0.5517241379310344]\n",
            "Test -  [0.6052631578947368, 0.6253623188405797, 0.5945945945945945]\n",
            "\n",
            "Epoch  58\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.4146908805705607\n",
            "Train -  [0.8991596638655462, 0.9638213680045223, 0.9]\n",
            "Validation -  [0.5666666666666667, 0.56, 0.48]\n",
            "Test -  [0.6052631578947368, 0.6282608695652173, 0.5833333333333334]\n",
            "\n",
            "Epoch  59\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.4473181050270796\n",
            "Train -  [0.9159663865546218, 0.958168456755229, 0.9166666666666666]\n",
            "Validation -  [0.5333333333333333, 0.5733333333333334, 0.4615384615384615]\n",
            "Test -  [0.6052631578947368, 0.6246376811594202, 0.5945945945945945]\n",
            "\n",
            "Epoch  60\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.3597046728245914\n",
            "Train -  [0.907563025210084, 0.9533634821933296, 0.9059829059829059]\n",
            "Validation -  [0.6, 0.6533333333333333, 0.5]\n",
            "Test -  [0.5789473684210527, 0.6543478260869565, 0.5675675675675677]\n",
            "\n",
            "Epoch  61\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.40052137337625027\n",
            "Train -  [0.9243697478991597, 0.9635387224420576, 0.9243697478991596]\n",
            "Validation -  [0.6, 0.5955555555555556, 0.5]\n",
            "Test -  [0.5921052631578947, 0.6014492753623188, 0.5753424657534246]\n",
            "\n",
            "Epoch  62\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.3942599957808852\n",
            "Train -  [0.9243697478991597, 0.9607122668174108, 0.923076923076923]\n",
            "Validation -  [0.6333333333333333, 0.6177777777777778, 0.5217391304347827]\n",
            "Test -  [0.5789473684210527, 0.6239130434782608, 0.5897435897435898]\n",
            "\n",
            "Epoch  63\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.40190626587718725\n",
            "Train -  [0.9159663865546218, 0.9689089881288864, 0.9152542372881356]\n",
            "Validation -  [0.5, 0.5022222222222222, 0.4]\n",
            "Test -  [0.618421052631579, 0.6427536231884057, 0.6027397260273971]\n",
            "\n",
            "Epoch  64\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.3422541862819344\n",
            "Train -  [0.907563025210084, 0.9686263425664217, 0.9075630252100839]\n",
            "Validation -  [0.5666666666666667, 0.591111111111111, 0.48]\n",
            "Test -  [0.6578947368421053, 0.6405797101449275, 0.6285714285714286]\n",
            "\n",
            "Epoch  65\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.31113277073018253\n",
            "Train -  [0.907563025210084, 0.9706048615036744, 0.9075630252100839]\n",
            "Validation -  [0.6, 0.5911111111111111, 0.5]\n",
            "Test -  [0.6052631578947368, 0.5978260869565217, 0.5945945945945945]\n",
            "\n",
            "Epoch  66\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.28456957964226604\n",
            "Train -  [0.8991596638655462, 0.9655172413793103, 0.8983050847457625]\n",
            "Validation -  [0.5333333333333333, 0.6444444444444445, 0.4166666666666667]\n",
            "Test -  [0.5789473684210527, 0.6, 0.5675675675675677]\n",
            "\n",
            "Epoch  67\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.3026964927557856\n",
            "Train -  [0.8991596638655462, 0.967495760316563, 0.8983050847457625]\n",
            "Validation -  [0.6, 0.6622222222222223, 0.5]\n",
            "Test -  [0.6052631578947368, 0.6782608695652174, 0.605263157894737]\n",
            "\n",
            "Epoch  68\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.2670117411762476\n",
            "Train -  [0.907563025210084, 0.9731486715658564, 0.9059829059829059]\n",
            "Validation -  [0.6, 0.6222222222222222, 0.5]\n",
            "Test -  [0.6052631578947368, 0.6304347826086957, 0.5588235294117647]\n",
            "\n",
            "Epoch  69\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.31271378975361586\n",
            "Train -  [0.9159663865546218, 0.9720180893159978, 0.9152542372881356]\n",
            "Validation -  [0.6333333333333333, 0.5733333333333334, 0.56]\n",
            "Test -  [0.5657894736842105, 0.6108695652173913, 0.547945205479452]\n",
            "\n",
            "Epoch  70\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.26189449429512024\n",
            "Train -  [0.9159663865546218, 0.9609949123798756, 0.9166666666666666]\n",
            "Validation -  [0.5333333333333333, 0.5733333333333334, 0.4166666666666667]\n",
            "Test -  [0.618421052631579, 0.6326086956521739, 0.5797101449275363]\n",
            "\n",
            "Epoch  71\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.2924630241468549\n",
            "Train -  [0.9327731092436975, 0.9762577727529678, 0.9322033898305084]\n",
            "Validation -  [0.6, 0.6044444444444445, 0.5]\n",
            "Test -  [0.5921052631578947, 0.6514492753623188, 0.5373134328358209]\n",
            "\n",
            "Epoch  72\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.2542013479396701\n",
            "Train -  [0.9243697478991597, 0.9725833804409271, 0.9243697478991596]\n",
            "Validation -  [0.6, 0.6711111111111111, 0.5]\n",
            "Test -  [0.6578947368421053, 0.6550724637681159, 0.6285714285714286]\n",
            "\n",
            "Epoch  73\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.3529886803589761\n",
            "Train -  [0.9243697478991597, 0.9737139626907857, 0.923076923076923]\n",
            "Validation -  [0.5333333333333333, 0.6, 0.36363636363636365]\n",
            "Test -  [0.618421052631579, 0.6478260869565218, 0.5245901639344263]\n",
            "\n",
            "Epoch  74\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.23069279454648495\n",
            "Train -  [0.9243697478991597, 0.9655172413793103, 0.923076923076923]\n",
            "Validation -  [0.5333333333333333, 0.5733333333333333, 0.4615384615384615]\n",
            "Test -  [0.5789473684210527, 0.5942028985507246, 0.5]\n",
            "\n",
            "Epoch  75\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.21981463441625237\n",
            "Train -  [0.907563025210084, 0.9592990390050876, 0.9043478260869565]\n",
            "Validation -  [0.5333333333333333, 0.5822222222222222, 0.4166666666666667]\n",
            "Test -  [0.5657894736842105, 0.6159420289855072, 0.5714285714285714]\n",
            "\n",
            "Epoch  76\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.26844962290488183\n",
            "Train -  [0.9243697478991597, 0.9646693046919164, 0.9243697478991596]\n",
            "Validation -  [0.5333333333333333, 0.5911111111111111, 0.4166666666666667]\n",
            "Test -  [0.6052631578947368, 0.6065217391304348, 0.5588235294117647]\n",
            "\n",
            "Epoch  77\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.22992169857025146\n",
            "Train -  [0.9159663865546218, 0.9717354437535329, 0.9137931034482758]\n",
            "Validation -  [0.6, 0.5866666666666667, 0.5]\n",
            "Test -  [0.618421052631579, 0.6340579710144927, 0.5538461538461538]\n",
            "\n",
            "Epoch  78\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.1900773928500712\n",
            "Train -  [0.9243697478991597, 0.9626907857546636, 0.923076923076923]\n",
            "Validation -  [0.5666666666666667, 0.5555555555555556, 0.43478260869565216]\n",
            "Test -  [0.6052631578947368, 0.5985507246376812, 0.5714285714285715]\n",
            "\n",
            "Epoch  79\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.18946116417646408\n",
            "Train -  [0.9159663865546218, 0.9655172413793104, 0.9152542372881356]\n",
            "Validation -  [0.5333333333333333, 0.5644444444444445, 0.36363636363636365]\n",
            "Test -  [0.618421052631579, 0.6637681159420289, 0.5915492957746479]\n",
            "\n",
            "Epoch  80\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.27152216667309403\n",
            "Train -  [0.9159663865546218, 0.9559072922555115, 0.9152542372881356]\n",
            "Validation -  [0.5333333333333333, 0.6, 0.36363636363636365]\n",
            "Test -  [0.631578947368421, 0.6615942028985506, 0.5757575757575758]\n",
            "\n",
            "Epoch  81\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.17945226177107543\n",
            "Train -  [0.9159663865546218, 0.9762577727529678, 0.9166666666666666]\n",
            "Validation -  [0.5, 0.5688888888888889, 0.3478260869565218]\n",
            "Test -  [0.5789473684210527, 0.5985507246376812, 0.5675675675675677]\n",
            "\n",
            "Epoch  82\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.1739553022198379\n",
            "Train -  [0.907563025210084, 0.9448841153193894, 0.9075630252100839]\n",
            "Validation -  [0.5, 0.5777777777777777, 0.3478260869565218]\n",
            "Test -  [0.5789473684210527, 0.6420289855072463, 0.5428571428571427]\n",
            "\n",
            "Epoch  83\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.2083587027154863\n",
            "Train -  [0.9159663865546218, 0.967495760316563, 0.9166666666666666]\n",
            "Validation -  [0.5333333333333333, 0.6177777777777778, 0.4615384615384615]\n",
            "Test -  [0.6447368421052632, 0.6826086956521739, 0.6197183098591549]\n",
            "\n",
            "Epoch  84\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.21436499012634158\n",
            "Train -  [0.9159663865546218, 0.9680610514414923, 0.9166666666666666]\n",
            "Validation -  [0.5333333333333333, 0.56, 0.4615384615384615]\n",
            "Test -  [0.6052631578947368, 0.6159420289855073, 0.5945945945945945]\n",
            "\n",
            "Epoch  85\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.19529907929245383\n",
            "Train -  [0.907563025210084, 0.9663651780667043, 0.9090909090909091]\n",
            "Validation -  [0.5666666666666667, 0.6044444444444445, 0.48]\n",
            "Test -  [0.5921052631578947, 0.6586956521739131, 0.5866666666666667]\n",
            "\n",
            "Epoch  86\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.24101174890529364\n",
            "Train -  [0.9159663865546218, 0.9601469756924815, 0.9166666666666666]\n",
            "Validation -  [0.5333333333333333, 0.5822222222222222, 0.4166666666666667]\n",
            "Test -  [0.618421052631579, 0.6608695652173914, 0.5671641791044775]\n",
            "\n",
            "Epoch  87\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.2537787859328091\n",
            "Train -  [0.9243697478991597, 0.9765404183154325, 0.9243697478991596]\n",
            "Validation -  [0.6, 0.64, 0.5]\n",
            "Test -  [0.6052631578947368, 0.6543478260869565, 0.5833333333333334]\n",
            "\n",
            "Epoch  88\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.2280423636548221\n",
            "Train -  [0.9243697478991597, 0.9748445449406443, 0.9243697478991596]\n",
            "Validation -  [0.5, 0.5911111111111111, 0.4]\n",
            "Test -  [0.6447368421052632, 0.6384057971014493, 0.5970149253731343]\n",
            "\n",
            "Epoch  89\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.14710114989429712\n",
            "Train -  [0.9243697478991597, 0.9737139626907857, 0.9243697478991596]\n",
            "Validation -  [0.5333333333333333, 0.5777777777777777, 0.4615384615384615]\n",
            "Test -  [0.6052631578947368, 0.6166666666666667, 0.5945945945945945]\n",
            "\n",
            "Epoch  90\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.1696994302328676\n",
            "Train -  [0.9327731092436975, 0.9754098360655737, 0.9322033898305084]\n",
            "Validation -  [0.6, 0.5733333333333334, 0.5]\n",
            "Test -  [0.5921052631578947, 0.5884057971014492, 0.5507246376811593]\n",
            "\n",
            "Epoch  91\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.15529810450971127\n",
            "Train -  [0.9243697478991597, 0.9737139626907858, 0.9243697478991596]\n",
            "Validation -  [0.6, 0.6, 0.5]\n",
            "Test -  [0.5789473684210527, 0.6369565217391304, 0.5555555555555556]\n",
            "\n",
            "Epoch  92\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.13558312272652984\n",
            "Train -  [0.9243697478991597, 0.9694742792538157, 0.9243697478991596]\n",
            "Validation -  [0.5333333333333333, 0.5688888888888889, 0.4166666666666667]\n",
            "Test -  [0.5657894736842105, 0.6731884057971015, 0.56]\n",
            "\n",
            "Epoch  93\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.18095700372941792\n",
            "Train -  [0.9243697478991597, 0.9674957603165629, 0.9243697478991596]\n",
            "Validation -  [0.5, 0.5599999999999999, 0.4]\n",
            "Test -  [0.618421052631579, 0.6376811594202898, 0.5671641791044775]\n",
            "\n",
            "Epoch  94\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.12011189246550202\n",
            "Train -  [0.9243697478991597, 0.9607122668174111, 0.9243697478991596]\n",
            "Validation -  [0.5333333333333333, 0.56, 0.4615384615384615]\n",
            "Test -  [0.5921052631578947, 0.5978260869565218, 0.5633802816901409]\n",
            "\n",
            "Epoch  95\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.1433201502659358\n",
            "Train -  [0.9243697478991597, 0.962408140192199, 0.9243697478991596]\n",
            "Validation -  [0.5, 0.5466666666666666, 0.3478260869565218]\n",
            "Test -  [0.6052631578947368, 0.6340579710144927, 0.5714285714285715]\n",
            "\n",
            "Epoch  96\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.10616236185887828\n",
            "Train -  [0.9327731092436975, 0.966647823629169, 0.9322033898305084]\n",
            "Validation -  [0.5333333333333333, 0.5555555555555556, 0.36363636363636365]\n",
            "Test -  [0.6052631578947368, 0.6144927536231883, 0.5454545454545454]\n",
            "\n",
            "Epoch  97\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.11064949119463563\n",
            "Train -  [0.9243697478991597, 0.9652345958168458, 0.9243697478991596]\n",
            "Validation -  [0.5666666666666667, 0.5777777777777778, 0.43478260869565216]\n",
            "Test -  [0.6710526315789473, 0.6804347826086957, 0.6031746031746033]\n",
            "\n",
            "Epoch  98\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.11017814005026594\n",
            "Train -  [0.9243697478991597, 0.9649519502543809, 0.9243697478991596]\n",
            "Validation -  [0.5666666666666667, 0.5822222222222222, 0.48]\n",
            "Test -  [0.618421052631579, 0.6405797101449276, 0.5797101449275363]\n",
            "\n",
            "Epoch  99\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  0.09193803212838247\n",
            "Train -  [0.8991596638655462, 0.9720180893159978, 0.8965517241379309]\n",
            "Validation -  [0.5333333333333333, 0.5288888888888889, 0.4166666666666667]\n",
            "Test -  [0.618421052631579, 0.6572463768115943, 0.5915492957746479]\n",
            "Epoch with best accuracy is 97\n",
            "'='*50 + 'END OF Bad epoch type + '='*50\n",
            "Good\n",
            "(651, 30, 2000) (651,)\n",
            "[-0.08427826 -0.08716451 -0.0831288  -0.07596581 -0.07719874]\n",
            "[1 2 1 2 1]\n",
            "start_idx_train: 80\n",
            "end_idx_train: 112\n",
            "start_idx_val: 112\n",
            "end_idx_val: 120\n",
            "start_idx_eval: 40\n",
            "end_idx_eval: 60\n",
            "[1 1 0 0 0 1 0 0 0 0]\n",
            "[0 1 0 1 0 0 1 1 0 0]\n",
            "[0 1 1 0 1 0 0 1 0 1]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Epoch  0\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  41.36527967453003\n",
            "Train -  [0.6153846153846154, 0.6280566280566281, 0.6491228070175438]\n",
            "Validation -  [0.44274809160305345, 0.4748251748251749, 0.45112781954887216]\n",
            "Test -  [0.5, 0.4934682964094729, 0.5474860335195532]\n",
            "\n",
            "Epoch  1\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  35.20172443985939\n",
            "Train -  [0.6057692307692307, 0.6932203139099691, 0.5610278372591007]\n",
            "Validation -  [0.5267175572519084, 0.49440559440559445, 0.48333333333333334]\n",
            "Test -  [0.5061728395061729, 0.5033231474407945, 0.4771241830065359]\n",
            "\n",
            "Epoch  2\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  34.04846006631851\n",
            "Train -  [0.6173076923076923, 0.705602153878016, 0.5756929637526652]\n",
            "Validation -  [0.48854961832061067, 0.47179487179487184, 0.48062015503875966]\n",
            "Test -  [0.5154320987654321, 0.5235676088617265, 0.49840255591054317]\n",
            "\n",
            "Epoch  3\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  31.50028294324875\n",
            "Train -  [0.6403846153846153, 0.7468305744167814, 0.589010989010989]\n",
            "Validation -  [0.5190839694656488, 0.45431235431235434, 0.4615384615384616]\n",
            "Test -  [0.4691358024691358, 0.4810542398777693, 0.40277777777777785]\n",
            "\n",
            "Epoch  4\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  30.69744399189949\n",
            "Train -  [0.698076923076923, 0.7853962336720958, 0.6609071274298057]\n",
            "Validation -  [0.5877862595419847, 0.524009324009324, 0.564516129032258]\n",
            "Test -  [0.4783950617283951, 0.49713521772345304, 0.44951140065146583]\n",
            "\n",
            "Epoch  5\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  29.701820850372314\n",
            "Train -  [0.7326923076923076, 0.837734286010148, 0.7048832271762208]\n",
            "Validation -  [0.5572519083969466, 0.5491841491841492, 0.5468749999999999]\n",
            "Test -  [0.5061728395061729, 0.5162337662337663, 0.4666666666666667]\n",
            "\n",
            "Epoch  6\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  28.093280643224716\n",
            "Train -  [0.7326923076923076, 0.8299826920516576, 0.7085953878406709]\n",
            "Validation -  [0.6335877862595419, 0.5927738927738927, 0.6065573770491803]\n",
            "Test -  [0.5216049382716049, 0.5355614973262033, 0.4884488448844884]\n",
            "\n",
            "Epoch  7\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  26.655382752418518\n",
            "Train -  [0.7634615384615384, 0.8645985887365197, 0.734341252699784]\n",
            "Validation -  [0.5114503816793893, 0.5025641025641026, 0.5076923076923077]\n",
            "Test -  [0.4845679012345679, 0.5093200916730328, 0.44518272425249167]\n",
            "\n",
            "Epoch  8\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  25.461076706647873\n",
            "Train -  [0.7557692307692307, 0.8715661474282164, 0.7280513918629551]\n",
            "Validation -  [0.5114503816793893, 0.5160839160839161, 0.5223880597014926]\n",
            "Test -  [0.5061728395061729, 0.5308632543926661, 0.47368421052631576]\n",
            "\n",
            "Epoch  9\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  24.111915081739426\n",
            "Train -  [0.7653846153846153, 0.8781047056909126, 0.7347826086956522]\n",
            "Validation -  [0.5572519083969466, 0.531934731934732, 0.5245901639344263]\n",
            "Test -  [0.5401234567901234, 0.5504965622612682, 0.5146579804560261]\n",
            "\n",
            "Epoch  10\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  22.3989260494709\n",
            "Train -  [0.7711538461538462, 0.8974393112324147, 0.7361419068736141]\n",
            "Validation -  [0.5572519083969466, 0.5533799533799535, 0.4912280701754386]\n",
            "Test -  [0.5154320987654321, 0.5312452253628724, 0.46048109965635736]\n",
            "\n",
            "Epoch  11\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  22.121809899806976\n",
            "Train -  [0.7942307692307692, 0.9107974970043935, 0.7606263982102909]\n",
            "Validation -  [0.5801526717557252, 0.5641025641025641, 0.5599999999999999]\n",
            "Test -  [0.5308641975308642, 0.5323911382734912, 0.4899328859060402]\n",
            "\n",
            "Epoch  12\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  20.82684889435768\n",
            "Train -  [0.825, 0.9166999511827097, 0.8067940552016986]\n",
            "Validation -  [0.5648854961832062, 0.5601398601398602, 0.5365853658536585]\n",
            "Test -  [0.5030864197530864, 0.5286478227654698, 0.48888888888888893]\n",
            "\n",
            "Epoch  13\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  19.048170268535614\n",
            "Train -  [0.8480769230769231, 0.9394369739197325, 0.841683366733467]\n",
            "Validation -  [0.549618320610687, 0.551981351981352, 0.5563909774436089]\n",
            "Test -  [0.5617283950617284, 0.5820473644003057, 0.5773809523809524]\n",
            "\n",
            "Epoch  14\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  19.422289073467255\n",
            "Train -  [0.8365384615384616, 0.9363452122072812, 0.8202959830866806]\n",
            "Validation -  [0.5725190839694656, 0.5461538461538461, 0.5555555555555556]\n",
            "Test -  [0.5, 0.531398013750955, 0.5]\n",
            "\n",
            "Epoch  15\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  17.574373587965965\n",
            "Train -  [0.85, 0.9409014926256306, 0.839506172839506]\n",
            "Validation -  [0.5419847328244275, 0.5328671328671328, 0.53125]\n",
            "Test -  [0.5401234567901234, 0.5437738731856379, 0.5415384615384616]\n",
            "\n",
            "Epoch  16\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  17.574436515569687\n",
            "Train -  [0.8384615384615385, 0.9276320655631001, 0.83399209486166]\n",
            "Validation -  [0.5343511450381679, 0.5491841491841493, 0.5413533834586466]\n",
            "Test -  [0.5555555555555556, 0.5974025974025975, 0.558282208588957]\n",
            "\n",
            "Epoch  17\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  18.303700014948845\n",
            "Train -  [0.8269230769230769, 0.9376765928490067, 0.8085106382978723]\n",
            "Validation -  [0.5114503816793893, 0.5277389277389278, 0.4482758620689655]\n",
            "Test -  [0.49382716049382713, 0.524446142093201, 0.48101265822784806]\n",
            "\n",
            "Epoch  18\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  16.400017023086548\n",
            "Train -  [0.8461538461538461, 0.9404724921966301, 0.8418972332015809]\n",
            "Validation -  [0.48854961832061067, 0.5403263403263404, 0.5179856115107914]\n",
            "Test -  [0.5030864197530864, 0.53231474407945, 0.513595166163142]\n",
            "\n",
            "Epoch  19\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  16.451414987444878\n",
            "Train -  [0.8788461538461538, 0.9553987485021967, 0.8722109533468559]\n",
            "Validation -  [0.5267175572519084, 0.562937062937063, 0.537313432835821]\n",
            "Test -  [0.5648148148148148, 0.594270435446906, 0.563467492260062]\n",
            "\n",
            "Epoch  20\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  15.225584767758846\n",
            "Train -  [0.8615384615384616, 0.9534608500125742, 0.8554216867469879]\n",
            "Validation -  [0.5419847328244275, 0.5543123543123544, 0.5774647887323944]\n",
            "Test -  [0.5462962962962963, 0.5729564553093964, 0.5739130434782609]\n",
            "\n",
            "Epoch  21\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  13.874176904559135\n",
            "Train -  [0.8653846153846154, 0.9519963313066762, 0.8653846153846154]\n",
            "Validation -  [0.5038167938931297, 0.5417249417249417, 0.5454545454545454]\n",
            "Test -  [0.5185185185185185, 0.554392666157372, 0.5568181818181818]\n",
            "\n",
            "Epoch  22\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  15.079087257385254\n",
            "Train -  [0.8730769230769231, 0.9475583958342579, 0.8764044943820225]\n",
            "Validation -  [0.5190839694656488, 0.5398601398601399, 0.5714285714285715]\n",
            "Test -  [0.5370370370370371, 0.5678762414056532, 0.5738636363636364]\n",
            "\n",
            "Epoch  23\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  13.76977638155222\n",
            "Train -  [0.8673076923076923, 0.9541709196881611, 0.873394495412844]\n",
            "Validation -  [0.46564885496183206, 0.5431235431235432, 0.5512820512820512]\n",
            "Test -  [0.5308641975308642, 0.5628724216959511, 0.5935828877005347]\n",
            "\n",
            "Epoch  24\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  13.724558681249619\n",
            "Train -  [0.8942307692307693, 0.9699403837334872, 0.8983364140480592]\n",
            "Validation -  [0.5343511450381679, 0.5671328671328674, 0.5850340136054422]\n",
            "Test -  [0.5679012345679012, 0.581283422459893, 0.6132596685082874]\n",
            "\n",
            "Epoch  25\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  12.7282135784626\n",
            "Train -  [0.8673076923076923, 0.9631799286971701, 0.8795811518324608]\n",
            "Validation -  [0.5343511450381679, 0.5766899766899767, 0.6064516129032258]\n",
            "Test -  [0.5401234567901234, 0.5599694423223834, 0.5962059620596206]\n",
            "\n",
            "Epoch  26\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  14.195423267781734\n",
            "Train -  [0.8942307692307693, 0.9689492448113138, 0.8923679060665363]\n",
            "Validation -  [0.549618320610687, 0.5890442890442891, 0.5815602836879432]\n",
            "Test -  [0.5524691358024691, 0.5813216195569136, 0.5747800586510263]\n",
            "\n",
            "Epoch  27\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  10.652311190962791\n",
            "Train -  [0.9057692307692308, 0.9696889007233835, 0.908411214953271]\n",
            "Validation -  [0.5190839694656488, 0.5452214452214452, 0.5771812080536913]\n",
            "Test -  [0.5308641975308642, 0.5639037433155081, 0.5706214689265537]\n",
            "\n",
            "Epoch  28\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  10.932416897267103\n",
            "Train -  [0.8903846153846153, 0.9757540792023551, 0.8829568788501028]\n",
            "Validation -  [0.5648854961832062, 0.5818181818181819, 0.5439999999999999]\n",
            "Test -  [0.5123456790123457, 0.5339954163483576, 0.5]\n",
            "\n",
            "Epoch  29\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  10.190554536879063\n",
            "Train -  [0.9096153846153846, 0.9758132516753207, 0.9061876247504991]\n",
            "Validation -  [0.48854961832061067, 0.5242424242424243, 0.49624060150375937]\n",
            "Test -  [0.5524691358024691, 0.5783804430863254, 0.5425867507886435]\n",
            "\n",
            "Epoch  30\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  10.796754360198975\n",
            "Train -  [0.9057692307692308, 0.9738161807127325, 0.9006085192697768]\n",
            "Validation -  [0.5572519083969466, 0.5806526806526807, 0.5468749999999999]\n",
            "Test -  [0.5740740740740741, 0.5901451489686784, 0.5576923076923077]\n",
            "\n",
            "Epoch  31\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  10.589294746518135\n",
            "Train -  [0.9096153846153846, 0.9741268361958016, 0.9101338432122371]\n",
            "Validation -  [0.5419847328244275, 0.5571095571095571, 0.5999999999999999]\n",
            "Test -  [0.5432098765432098, 0.5824484339190221, 0.569767441860465]\n",
            "\n",
            "Epoch  32\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.9549794010818\n",
            "Train -  [0.9019230769230769, 0.9759907690942173, 0.8965517241379309]\n",
            "Validation -  [0.5038167938931297, 0.5592074592074592, 0.48000000000000004]\n",
            "Test -  [0.5432098765432098, 0.5837662337662338, 0.5286624203821655]\n",
            "\n",
            "Epoch  33\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  9.27115535736084\n",
            "Train -  [0.8826923076923077, 0.9717895235136614, 0.8721174004192872]\n",
            "Validation -  [0.5190839694656488, 0.5263403263403263, 0.4878048780487805]\n",
            "Test -  [0.5370370370370371, 0.5712757830404889, 0.4863013698630137]\n",
            "\n",
            "Epoch  34\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.497127655893564\n",
            "Train -  [0.9211538461538461, 0.9820263613367062, 0.9175050301810865]\n",
            "Validation -  [0.5801526717557252, 0.5813519813519815, 0.5736434108527132]\n",
            "Test -  [0.5493827160493827, 0.5990450725744844, 0.5228758169934641]\n",
            "\n",
            "Epoch  35\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.782268421724439\n",
            "Train -  [0.9019230769230769, 0.9745854228612849, 0.899009900990099]\n",
            "Validation -  [0.5267175572519084, 0.5701631701631702, 0.537313432835821]\n",
            "Test -  [0.5555555555555556, 0.5613063407181054, 0.5662650602409638]\n",
            "\n",
            "Epoch  36\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  10.342917747795582\n",
            "Train -  [0.8961538461538462, 0.9741120430775603, 0.891566265060241]\n",
            "Validation -  [0.5267175572519084, 0.5627039627039626, 0.5000000000000001]\n",
            "Test -  [0.5462962962962963, 0.5725744843391902, 0.5303514376996805]\n",
            "\n",
            "Epoch  37\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.525644861161709\n",
            "Train -  [0.925, 0.9823813961744996, 0.9254302103250478]\n",
            "Validation -  [0.549618320610687, 0.5445221445221445, 0.5815602836879432]\n",
            "Test -  [0.5740740740740741, 0.6145912910618794, 0.5964912280701755]\n",
            "\n",
            "Epoch  38\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  9.404544327408075\n",
            "Train -  [0.9076923076923077, 0.972336868888593, 0.9111111111111111]\n",
            "Validation -  [0.5190839694656488, 0.5666666666666667, 0.5594405594405595]\n",
            "Test -  [0.5308641975308642, 0.5568754774637127, 0.5681818181818182]\n",
            "\n",
            "Epoch  39\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.182762634940445\n",
            "Train -  [0.926923076923077, 0.9790825308066687, 0.9263565891472869]\n",
            "Validation -  [0.48854961832061067, 0.5177156177156177, 0.5037037037037038]\n",
            "Test -  [0.5462962962962963, 0.5610007639419403, 0.5637982195845698]\n",
            "\n",
            "Epoch  40\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.013888800516725\n",
            "Train -  [0.9096153846153846, 0.9761387002766313, 0.9124767225325885]\n",
            "Validation -  [0.46564885496183206, 0.48857808857808854, 0.5]\n",
            "Test -  [0.5524691358024691, 0.5886172650878534, 0.596100278551532]\n",
            "\n",
            "Epoch  41\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  7.926208203658462\n",
            "Train -  [0.9192307692307692, 0.9836240181067767, 0.9153225806451613]\n",
            "Validation -  [0.5725190839694656, 0.5508158508158508, 0.582089552238806]\n",
            "Test -  [0.5246913580246914, 0.5646676852559205, 0.5246913580246914]\n",
            "\n",
            "Epoch  42\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.121717024594545\n",
            "Train -  [0.926923076923077, 0.984348880900605, 0.9224489795918368]\n",
            "Validation -  [0.5190839694656488, 0.5233100233100233, 0.4878048780487805]\n",
            "Test -  [0.5555555555555556, 0.5716959511077159, 0.5231788079470199]\n",
            "\n",
            "Epoch  43\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.033858919516206\n",
            "Train -  [0.9192307692307692, 0.9801328422018077, 0.9198473282442747]\n",
            "Validation -  [0.5038167938931297, 0.5342657342657343, 0.5323741007194245]\n",
            "Test -  [0.5216049382716049, 0.5415966386554623, 0.5373134328358209]\n",
            "\n",
            "Epoch  44\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.255126190371811\n",
            "Train -  [0.9211538461538461, 0.9757096998476309, 0.9224952741020793]\n",
            "Validation -  [0.5419847328244275, 0.536946386946387, 0.5774647887323944]\n",
            "Test -  [0.5833333333333334, 0.5949197860962566, 0.599406528189911]\n",
            "\n",
            "Epoch  45\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.513218412175775\n",
            "Train -  [0.8942307692307693, 0.9767304250062872, 0.8837209302325583]\n",
            "Validation -  [0.5725190839694656, 0.5666666666666667, 0.5254237288135594]\n",
            "Test -  [0.5308641975308642, 0.5733766233766233, 0.49333333333333335]\n",
            "\n",
            "Epoch  46\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  9.442890156060457\n",
            "Train -  [0.9115384615384615, 0.9785647716682199, 0.908]\n",
            "Validation -  [0.5572519083969466, 0.5659673659673661, 0.5671641791044777]\n",
            "Test -  [0.5617283950617284, 0.580977845683728, 0.5590062111801242]\n",
            "\n",
            "Epoch  47\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.290881721302867\n",
            "Train -  [0.9365384615384615, 0.9888459888459888, 0.9346534653465346]\n",
            "Validation -  [0.48854961832061067, 0.5145687645687645, 0.464]\n",
            "Test -  [0.5308641975308642, 0.5676852559205501, 0.5220125786163522]\n",
            "\n",
            "Epoch  48\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.41001608595252\n",
            "Train -  [0.9403846153846154, 0.98408260477226, 0.9386138613861386]\n",
            "Validation -  [0.549618320610687, 0.5708624708624709, 0.5693430656934306]\n",
            "Test -  [0.5555555555555556, 0.5993506493506493, 0.558282208588957]\n",
            "\n",
            "Epoch  49\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  8.12662999611348\n",
            "Train -  [0.9115384615384615, 0.9803103596207045, 0.905349794238683]\n",
            "Validation -  [0.5267175572519084, 0.5510489510489511, 0.5000000000000001]\n",
            "Test -  [0.49382716049382713, 0.5479373567608862, 0.4421768707482993]\n",
            "\n",
            "Epoch  50\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  7.615075992420316\n",
            "Train -  [0.9461538461538461, 0.9862571931537448, 0.9461538461538461]\n",
            "Validation -  [0.48854961832061067, 0.5445221445221445, 0.5037037037037038]\n",
            "Test -  [0.5648148148148148, 0.5704354469060351, 0.566153846153846]\n",
            "\n",
            "Epoch  51\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  7.727987553924322\n",
            "Train -  [0.9576923076923077, 0.9901034038965074, 0.9575289575289575]\n",
            "Validation -  [0.549618320610687, 0.5754079254079255, 0.5815602836879432]\n",
            "Test -  [0.5648148148148148, 0.5773491214667685, 0.591304347826087]\n",
            "\n",
            "Epoch  52\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.867883497849107\n",
            "Train -  [0.9326923076923077, 0.9872779183124011, 0.9353049907578558]\n",
            "Validation -  [0.5343511450381679, 0.5642191142191143, 0.5906040268456375]\n",
            "Test -  [0.5462962962962963, 0.5733193277310924, 0.5859154929577466]\n",
            "\n",
            "Epoch  53\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.189999297261238\n",
            "Train -  [0.9403846153846154, 0.9890087131466442, 0.9395711500974658]\n",
            "Validation -  [0.5801526717557252, 0.5783216783216784, 0.5925925925925927]\n",
            "Test -  [0.5617283950617284, 0.5635217723453018, 0.5644171779141104]\n",
            "\n",
            "Epoch  54\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.261320831254125\n",
            "Train -  [0.9365384615384615, 0.9874554357312979, 0.934131736526946]\n",
            "Validation -  [0.5190839694656488, 0.5517482517482518, 0.5263157894736842]\n",
            "Test -  [0.5493827160493827, 0.5777310924369748, 0.5379746835443039]\n",
            "\n",
            "Epoch  55\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.6090349880978465\n",
            "Train -  [0.9307692307692308, 0.9839198804716045, 0.9277108433734939]\n",
            "Validation -  [0.5343511450381679, 0.5202797202797202, 0.534351145038168]\n",
            "Test -  [0.5370370370370371, 0.5702826585179526, 0.53125]\n",
            "\n",
            "Epoch  56\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.487656264565885\n",
            "Train -  [0.9365384615384615, 0.9847335019748813, 0.9359223300970874]\n",
            "Validation -  [0.6030534351145038, 0.5864801864801865, 0.6]\n",
            "Test -  [0.5462962962962963, 0.5725744843391902, 0.5333333333333333]\n",
            "\n",
            "Epoch  57\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.5939222648739815\n",
            "Train -  [0.9192307692307692, 0.9817009127353955, 0.9169960474308301]\n",
            "Validation -  [0.5343511450381679, 0.5407925407925408, 0.534351145038168]\n",
            "Test -  [0.5740740740740741, 0.5921504965622613, 0.56875]\n",
            "\n",
            "Epoch  58\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.173119190149009\n",
            "Train -  [0.9461538461538461, 0.9895264722850929, 0.9448818897637796]\n",
            "Validation -  [0.5801526717557252, 0.5913752913752913, 0.5925925925925927]\n",
            "Test -  [0.5709876543209876, 0.5856569900687547, 0.5899705014749262]\n",
            "\n",
            "Epoch  59\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.050463180523366\n",
            "Train -  [0.9403846153846154, 0.9839938460628117, 0.9393346379647749]\n",
            "Validation -  [0.5343511450381679, 0.5363636363636364, 0.5547445255474452]\n",
            "Test -  [0.5401234567901234, 0.5637318563789152, 0.5329153605015674]\n",
            "\n",
            "Epoch  60\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.504842198453844\n",
            "Train -  [0.95, 0.9901181970147487, 0.9498069498069498]\n",
            "Validation -  [0.5267175572519084, 0.5361305361305362, 0.5507246376811594]\n",
            "Test -  [0.5185185185185185, 0.5673414820473643, 0.5329341317365268]\n",
            "\n",
            "Epoch  61\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.654167332686484\n",
            "Train -  [0.9384615384615385, 0.9849923815441057, 0.9391634980988595]\n",
            "Validation -  [0.5877862595419847, 0.5686480186480186, 0.619718309859155]\n",
            "Test -  [0.5370370370370371, 0.5664820473644003, 0.5562130177514792]\n",
            "\n",
            "Epoch  62\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.46328873699531\n",
            "Train -  [0.95, 0.9887868163730233, 0.950381679389313]\n",
            "Validation -  [0.5419847328244275, 0.5517482517482517, 0.5714285714285715]\n",
            "Test -  [0.5617283950617284, 0.5673414820473643, 0.5773809523809524]\n",
            "\n",
            "Epoch  63\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.573529385961592\n",
            "Train -  [0.9442307692307692, 0.9855767097146407, 0.9421157684630739]\n",
            "Validation -  [0.5648854961832062, 0.5475524475524476, 0.5365853658536585]\n",
            "Test -  [0.5308641975308642, 0.5746753246753247, 0.5]\n",
            "\n",
            "Epoch  64\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.783642661524937\n",
            "Train -  [0.9384615384615385, 0.9818488439178095, 0.9375]\n",
            "Validation -  [0.5648854961832062, 0.5766899766899767, 0.5714285714285715]\n",
            "Test -  [0.5370370370370371, 0.5776928953399542, 0.5426829268292682]\n",
            "\n",
            "Epoch  65\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.785745835164562\n",
            "Train -  [0.9346153846153846, 0.9862571931537449, 0.9322709163346614]\n",
            "Validation -  [0.5648854961832062, 0.5615384615384614, 0.5581395348837209]\n",
            "Test -  [0.558641975308642, 0.5791825821237586, 0.5431309904153354]\n",
            "\n",
            "Epoch  66\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.995165311032906\n",
            "Train -  [0.9346153846153846, 0.9864199174544002, 0.932]\n",
            "Validation -  [0.5725190839694656, 0.5513986013986014, 0.5625]\n",
            "Test -  [0.5185185185185185, 0.5653934300993124, 0.5063291139240506]\n",
            "\n",
            "Epoch  67\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.243286328273825\n",
            "Train -  [0.9192307692307692, 0.9793488069350138, 0.9149797570850202]\n",
            "Validation -  [0.5572519083969466, 0.5441724941724941, 0.5396825396825397]\n",
            "Test -  [0.5555555555555556, 0.5653552330022918, 0.5384615384615384]\n",
            "\n",
            "Epoch  68\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  7.017779205925763\n",
            "Train -  [0.9, 0.98217429251912, 0.891213389121339]\n",
            "Validation -  [0.5801526717557252, 0.5501165501165501, 0.5454545454545454]\n",
            "Test -  [0.5524691358024691, 0.6008976317799847, 0.5182724252491694]\n",
            "\n",
            "Epoch  69\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.272167592658661\n",
            "Train -  [0.9153846153846154, 0.9865826417550556, 0.9079497907949791]\n",
            "Validation -  [0.5572519083969466, 0.5406759906759907, 0.5166666666666667]\n",
            "Test -  [0.5154320987654321, 0.550305576776165, 0.4713804713804714]\n",
            "\n",
            "Epoch  70\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.4172810120508075\n",
            "Train -  [0.9461538461538461, 0.9867305729374695, 0.9446640316205535]\n",
            "Validation -  [0.5343511450381679, 0.5715617715617717, 0.5196850393700787]\n",
            "Test -  [0.5740740740740741, 0.5971352177234531, 0.5460526315789473]\n",
            "\n",
            "Epoch  71\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.676802495494485\n",
            "Train -  [0.9346153846153846, 0.986671400464504, 0.932]\n",
            "Validation -  [0.6030534351145038, 0.5813519813519814, 0.59375]\n",
            "Test -  [0.558641975308642, 0.5622230710466005, 0.5372168284789643]\n",
            "\n",
            "Epoch  72\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.887787840678357\n",
            "Train -  [0.9365384615384615, 0.9839790529445703, 0.9346534653465346]\n",
            "Validation -  [0.5572519083969466, 0.563053613053613, 0.5735294117647058]\n",
            "Test -  [0.5432098765432098, 0.5787433155080214, 0.5487804878048781]\n",
            "\n",
            "Epoch  73\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.176664578844793\n",
            "Train -  [0.9230769230769231, 0.9828843621947071, 0.9267399267399268]\n",
            "Validation -  [0.5877862595419847, 0.5748251748251748, 0.625]\n",
            "Test -  [0.5462962962962963, 0.5736058059587472, 0.5714285714285714]\n",
            "\n",
            "Epoch  74\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.4463755544275045\n",
            "Train -  [0.9288461538461539, 0.9769079424251839, 0.9246435845213848]\n",
            "Validation -  [0.5267175572519084, 0.5375291375291374, 0.48333333333333334]\n",
            "Test -  [0.5339506172839507, 0.5443468296409473, 0.49498327759197336]\n",
            "\n",
            "Epoch  75\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.177601933246478\n",
            "Train -  [0.9557692307692308, 0.9869820559475732, 0.9558541266794626]\n",
            "Validation -  [0.5725190839694656, 0.5578088578088578, 0.5942028985507246]\n",
            "Test -  [0.5401234567901234, 0.5680099312452254, 0.5578635014836795]\n",
            "\n",
            "Epoch  76\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.8643351891078055\n",
            "Train -  [0.9403846153846154, 0.9865382624003314, 0.9386138613861386]\n",
            "Validation -  [0.5419847328244275, 0.5699300699300699, 0.5454545454545454]\n",
            "Test -  [0.5432098765432098, 0.5778265851795263, 0.5432098765432098]\n",
            "\n",
            "Epoch  77\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.179816020885482\n",
            "Train -  [0.95, 0.988550126481161, 0.9484126984126985]\n",
            "Validation -  [0.5343511450381679, 0.5276223776223776, 0.5196850393700787]\n",
            "Test -  [0.5401234567901234, 0.5717532467532467, 0.5239616613418531]\n",
            "\n",
            "Epoch  78\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.550342520698905\n",
            "Train -  [0.948076923076923, 0.9885723161585231, 0.9475728155339807]\n",
            "Validation -  [0.5419847328244275, 0.5370629370629372, 0.5588235294117647]\n",
            "Test -  [0.5555555555555556, 0.5651451489686784, 0.5471698113207548]\n",
            "\n",
            "Epoch  79\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.182208446785808\n",
            "Train -  [0.948076923076923, 0.9854657613278303, 0.9481765834932822]\n",
            "Validation -  [0.4961832061068702, 0.5193473193473194, 0.5074626865671642]\n",
            "Test -  [0.5555555555555556, 0.5623949579831933, 0.558282208588957]\n",
            "\n",
            "Epoch  80\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.00935396249406\n",
            "Train -  [0.9596153846153846, 0.9911980946463705, 0.9598470363288719]\n",
            "Validation -  [0.5801526717557252, 0.5856643356643357, 0.5985401459854014]\n",
            "Test -  [0.5648148148148148, 0.5818563789152025, 0.5791044776119403]\n",
            "\n",
            "Epoch  81\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.299211440258659\n",
            "Train -  [0.9576923076923077, 0.9908726460450599, 0.9573643410852712]\n",
            "Validation -  [0.5572519083969466, 0.5621212121212122, 0.5671641791044777]\n",
            "Test -  [0.5493827160493827, 0.5739877769289534, 0.5654761904761905]\n",
            "\n",
            "Epoch  82\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  6.945265085902065\n",
            "Train -  [0.9403846153846154, 0.9838755011168805, 0.940952380952381]\n",
            "Validation -  [0.5267175572519084, 0.5550116550116551, 0.557142857142857]\n",
            "Test -  [0.5555555555555556, 0.5553475935828877, 0.5838150289017341]\n",
            "\n",
            "Epoch  83\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.694165894761682\n",
            "Train -  [0.9403846153846154, 0.9852216748768473, 0.9386138613861386]\n",
            "Validation -  [0.5114503816793893, 0.5255244755244755, 0.4920634920634921]\n",
            "Test -  [0.5432098765432098, 0.5495225362872421, 0.509933774834437]\n",
            "\n",
            "Epoch  84\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.828193128865678\n",
            "Train -  [0.9365384615384615, 0.984911019393778, 0.9351669941060904]\n",
            "Validation -  [0.5419847328244275, 0.5120046620046621, 0.5454545454545454]\n",
            "Test -  [0.558641975308642, 0.5870702826585179, 0.5545171339563864]\n",
            "\n",
            "Epoch  85\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.013820096501149\n",
            "Train -  [0.9519230769230769, 0.9909318185180255, 0.952919020715631]\n",
            "Validation -  [0.5648854961832062, 0.5757575757575758, 0.6122448979591837]\n",
            "Test -  [0.5925925925925926, 0.6036478227654698, 0.625]\n",
            "\n",
            "Epoch  86\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.508079537656158\n",
            "Train -  [0.9519230769230769, 0.9864864864864865, 0.9514563106796116]\n",
            "Validation -  [0.48854961832061067, 0.5064102564102564, 0.4724409448818898]\n",
            "Test -  [0.5648148148148148, 0.5846829640947289, 0.5740181268882175]\n",
            "\n",
            "Epoch  87\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.808949459344149\n",
            "Train -  [0.926923076923077, 0.9798517729552212, 0.9260700389105058]\n",
            "Validation -  [0.5572519083969466, 0.5567599067599067, 0.5606060606060606]\n",
            "Test -  [0.5493827160493827, 0.5684300993124523, 0.562874251497006]\n",
            "\n",
            "Epoch  88\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.868099419400096\n",
            "Train -  [0.8923076923076924, 0.9867601591739523, 0.9024390243902439]\n",
            "Validation -  [0.5419847328244275, 0.5515151515151515, 0.6103896103896104]\n",
            "Test -  [0.5709876543209876, 0.5915584415584416, 0.6351706036745407]\n",
            "\n",
            "Epoch  89\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.623161129769869\n",
            "Train -  [0.9519230769230769, 0.9912720602375775, 0.9536178107606679]\n",
            "Validation -  [0.5343511450381679, 0.538111888111888, 0.5734265734265734]\n",
            "Test -  [0.5462962962962963, 0.5847975553857907, 0.5811965811965812]\n",
            "\n",
            "Epoch  90\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.297004982596263\n",
            "Train -  [0.9538461538461539, 0.9863163656267105, 0.9534883720930233]\n",
            "Validation -  [0.5190839694656488, 0.5242424242424243, 0.5401459854014599]\n",
            "Test -  [0.5432098765432098, 0.5680481283422459, 0.5542168674698795]\n",
            "\n",
            "Epoch  91\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  3.461510075023398\n",
            "Train -  [0.9461538461538461, 0.9866861935827453, 0.9477611940298508]\n",
            "Validation -  [0.549618320610687, 0.545920745920746, 0.5874125874125874]\n",
            "Test -  [0.5370370370370371, 0.5622994652406418, 0.5810055865921787]\n",
            "\n",
            "Epoch  92\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.756981740472838\n",
            "Train -  [0.9461538461538461, 0.990014645187059, 0.9473684210526315]\n",
            "Validation -  [0.4961832061068702, 0.5251748251748252, 0.547945205479452]\n",
            "Test -  [0.5709876543209876, 0.5730328495034378, 0.6106442577030813]\n",
            "\n",
            "Epoch  93\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  3.8453984170337208\n",
            "Train -  [0.9596153846153846, 0.9907690942173701, 0.9596928982725528]\n",
            "Validation -  [0.5343511450381679, 0.5613053613053613, 0.5793103448275863]\n",
            "Test -  [0.5524691358024691, 0.5604469060351414, 0.5797101449275363]\n",
            "\n",
            "Epoch  94\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  3.5536570558906533\n",
            "Train -  [0.9403846153846154, 0.9857838133700202, 0.9420560747663552]\n",
            "Validation -  [0.5419847328244275, 0.5534965034965035, 0.589041095890411]\n",
            "Test -  [0.5648148148148148, 0.5811115355233003, 0.6028169014084507]\n",
            "\n",
            "Epoch  95\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  2.916892643435858\n",
            "Train -  [0.9634615384615385, 0.9921004748590956, 0.9639468690702087]\n",
            "Validation -  [0.5572519083969466, 0.5594405594405594, 0.5915492957746479]\n",
            "Test -  [0.5802469135802469, 0.5757639419404125, 0.615819209039548]\n",
            "\n",
            "Epoch  96\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  3.457847402431071\n",
            "Train -  [0.9173076923076923, 0.9880989363747984, 0.9233511586452763]\n",
            "Validation -  [0.5343511450381679, 0.5235431235431236, 0.6013071895424837]\n",
            "Test -  [0.5833333333333334, 0.6185637891520244, 0.6475195822454307]\n",
            "\n",
            "Epoch  97\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.06153297284618\n",
            "Train -  [0.948076923076923, 0.9857542271335374, 0.949343339587242]\n",
            "Validation -  [0.5343511450381679, 0.534032634032634, 0.5793103448275863]\n",
            "Test -  [0.5617283950617284, 0.5789533995416348, 0.6011235955056181]\n",
            "\n",
            "Epoch  98\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  4.647147378651425\n",
            "Train -  [0.948076923076923, 0.9887498335774197, 0.9502762430939227]\n",
            "Validation -  [0.5267175572519084, 0.5452214452214452, 0.5921052631578947]\n",
            "Test -  [0.5555555555555556, 0.5881207028265851, 0.6149732620320856]\n",
            "\n",
            "Epoch  99\n",
            "['acc', 'auc', 'fmeasure']\n",
            "Training Loss  5.5386161491042\n",
            "Train -  [0.9288461538461539, 0.9865530555185728, 0.9303201506591338]\n",
            "Validation -  [0.5572519083969466, 0.5441724941724942, 0.5606060606060606]\n",
            "Test -  [0.5216049382716049, 0.5506875477463713, 0.540059347181009]\n",
            "Epoch with best accuracy is 85\n",
            "'='*50 + 'END OF Good epoch type + '='*50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list_train_performance[:,0])\n",
        "plt.plot(list_valid_performance[:,0])\n",
        "plt.plot(list_test_performance[:,0])\n",
        "plt.title(\"EEGNET's performance on all epochs\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2CmKPWccWZXm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "30f9d933-998d-40ad-ccc6-5fd091981a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_test_types = np.array(best_test_types)\n",
        "best_test_types"
      ],
      "metadata": {
        "id": "87rW6KuYkps2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56f7ae2-342c-48cf-95b0-9333475ebb84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.63      , 0.64505   , 0.61256545],\n",
              "       [0.67105263, 0.68043478, 0.6031746 ],\n",
              "       [0.59259259, 0.60364782, 0.625     ]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_test_types.shape"
      ],
      "metadata": {
        "id": "PDQvDlGBEGqv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad565b2-b405-4985-d4c3-dedeb19bb21f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save data"
      ],
      "metadata": {
        "id": "MRWWTOMImvDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change to Percentage\n",
        "best_test_types = best_test_types * 100\n",
        "# Create DataFrame\n",
        "df_model_metrics = pd.DataFrame(data=best_test_types,\n",
        "                      index= list_epoch_types,\n",
        "                      columns=params)\n",
        "\n",
        "df_model_metrics"
      ],
      "metadata": {
        "id": "5L-en6kWmRAn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "a5aa0cd8-c0c4-4572-f735-e0244deb08ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  acc        auc   fmeasure\n",
              "All epochs  63.000000  64.505000  61.256545\n",
              "Bad         67.105263  68.043478  60.317460\n",
              "Good        59.259259  60.364782  62.500000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2cae18e5-b48d-459e-8176-393a9c1394c6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc</th>\n",
              "      <th>auc</th>\n",
              "      <th>fmeasure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>All epochs</th>\n",
              "      <td>63.000000</td>\n",
              "      <td>64.505000</td>\n",
              "      <td>61.256545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bad</th>\n",
              "      <td>67.105263</td>\n",
              "      <td>68.043478</td>\n",
              "      <td>60.317460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Good</th>\n",
              "      <td>59.259259</td>\n",
              "      <td>60.364782</td>\n",
              "      <td>62.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2cae18e5-b48d-459e-8176-393a9c1394c6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2cae18e5-b48d-459e-8176-393a9c1394c6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2cae18e5-b48d-459e-8176-393a9c1394c6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{EOG_ref}-{ECG_ref}\")"
      ],
      "metadata": {
        "id": "xsladmNhw6kE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9518dd71-80a4-4647-d59c-2dd6ad670c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EEGANet-None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df_model_metrics.to_csv(path_or_buf=f'result/MI/MI_GoodBad_EEGNET-{EOG_ref}-{ECG_ref}.csv', \n",
        "#                         sep=',', float_format=None)"
      ],
      "metadata": {
        "id": "0_srsju8XIRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xLt5lACdZb-5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}