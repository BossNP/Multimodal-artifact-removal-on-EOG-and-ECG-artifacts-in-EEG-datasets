{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoVunR-YfYao"
      },
      "source": [
        "# In case, the file import data from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ6JGzB4fpzV",
        "outputId": "0a397956-0798-4a44-adb3-b526ee872e4b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OgetZCcfr_a",
        "outputId": "727629cc-a290-4e1b-c17b-ea981f0e316d"
      },
      "source": [
        "# Install libraries\n",
        "!pip install mne\n",
        "!pip install pyriemann"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mne\n",
            "  Downloading mne-0.24.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.4.1)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-0.24.0\n",
            "Collecting pyriemann\n",
            "  Downloading pyriemann-0.2.7.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 447 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyriemann) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyriemann) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyriemann) (1.0.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyriemann) (1.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pyriemann) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pyriemann) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->pyriemann) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->pyriemann) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyriemann) (3.0.0)\n",
            "Building wheels for collected packages: pyriemann\n",
            "  Building wheel for pyriemann (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyriemann: filename=pyriemann-0.2.7-py2.py3-none-any.whl size=49770 sha256=c33e5c7e568853410bd8b538a9f1b03b95f85e0520e41a65c9c2c07c900eb6bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/b7/55/27dcb08ed8fb58da8c1be108c23928ffb9125c9c1da2ddfb53\n",
            "Successfully built pyriemann\n",
            "Installing collected packages: pyriemann\n",
            "Successfully installed pyriemann-0.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxUttm0PfwLI"
      },
      "source": [
        "# Reference paper\n",
        "- A multi-subject, multi-modal human neuroimaging dataset\n",
        "    - https://github.com/bids-standard/bids-examples/tree/master/ds000117\n",
        "    - https://openneuro.org/datasets/ds000117/versions/1.0.5\n",
        "    - https://www.nature.com/articles/sdata20151\n",
        "- Notes\n",
        "    - Rename and type of channels\n",
        "        - EEG061 = HEOG\n",
        "        - EEG062 = VEOG\n",
        "        - EEG063 = ECG\n",
        "    - In README\n",
        "        - There is an explanation of triggers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tciqSn9gPh7"
      },
      "source": [
        "# Import libraries and read files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Raw1IZobgPes"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import mne\n",
        "from mne import find_events, Epochs, pick_types, read_evokeds\n",
        "from mne.preprocessing import ICA\n",
        "\n",
        "import pywt\n",
        "import scipy\n",
        "from mne.preprocessing import (ICA, create_eog_epochs, create_ecg_epochs,\n",
        "                               corrmap)\n",
        "from sklearn.neighbors import KDTree\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzCeFKr75MMM"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0wD5a9T5LwA"
      },
      "source": [
        "\"\"\"\n",
        "Calling a function of artifact removal technique according to \n",
        "EOG_ref and ECG_ref parameters\n",
        "\n",
        "EOG_ref has 4 options\n",
        "1) None\n",
        "2) 'wICA_without_ref'\n",
        "3) 'wICA_with_ref'\n",
        "4) 'EEGANet'\n",
        "\n",
        "ECG_ref has 3 options\n",
        "1) None\n",
        "2) ICA_with_ref\n",
        "3) wICA_with_ref\n",
        "\"\"\"\n",
        "\n",
        "sfreq_after_epoch = 200\n",
        "EOG_ref = 'wICA_with_ref'\n",
        "ECG_ref = 'ICA_with_ref'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0TtNyungPbT"
      },
      "source": [
        "ch_name_dic ={'EEG001': 'PO9', 'EEG002': 'Fpz', 'EEG003': 'PO10', 'EEG004': 'AF7', \n",
        "              'EEG005': 'AF3', 'EEG006': 'AFz', 'EEG007': 'AF4', 'EEG008': 'AF8', 'EEG009': 'F7', \n",
        "              'EEG010': 'F5', 'EEG011': 'F3', 'EEG012': 'F1', 'EEG013': 'Fz', 'EEG014': 'F2', \n",
        "              'EEG015': 'F4', 'EEG016': 'F6', 'EEG017': 'F8', 'EEG018': 'FT9', 'EEG019': 'FT7', \n",
        "              'EEG020': 'FC5', 'EEG021': 'FC3', 'EEG022': 'FC1', 'EEG023': 'FCz', 'EEG024': 'FC2', \n",
        "              'EEG025': 'FC4', 'EEG026': 'FC6', 'EEG027': 'FT8', 'EEG028': 'FT10', 'EEG029': 'T9', \n",
        "              'EEG030': 'T7', 'EEG031': 'C5', 'EEG032': 'C3', 'EEG033': 'C1', 'EEG034': 'Cz', \n",
        "              'EEG035': 'C2', 'EEG036': 'C4', 'EEG037': 'C6', 'EEG038': 'T8', 'EEG039': 'T10', \n",
        "              'EEG040': 'TP9', 'EEG041': 'TP7', 'EEG042': 'CP5', 'EEG043': 'CP3', 'EEG044': 'CP1', \n",
        "              'EEG045': 'CPz', 'EEG046': 'CP2', 'EEG047': 'CP4', 'EEG048': 'CP6', 'EEG049': 'TP8', \n",
        "              'EEG050': 'TP10', 'EEG051': 'P9', 'EEG052': 'P7', 'EEG053': 'P5', 'EEG054': 'P3', \n",
        "              'EEG055': 'P1', 'EEG056': 'Pz', 'EEG057': 'P2', 'EEG058': 'P4', 'EEG059': 'P6', \n",
        "              'EEG060': 'P8', 'EEG065': 'P10', 'EEG066': 'PO7', 'EEG067': 'PO3', 'EEG068': 'POz',  \n",
        "              'EEG069': 'PO4', 'EEG070': 'PO8', 'EEG071': 'O1', 'EEG072': 'Oz', 'EEG073': 'O2', 'EEG074': 'Iz'}\n",
        "\n",
        "event_id = {'Famous':0,'Unfamiliar':1,'Scrambled':2} "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2aSAUMbSNX6"
      },
      "source": [
        "def compute_kurtosis(data):\n",
        "    \n",
        "    \"\"\"Kurtosis of the data (per channel).\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : ndarray, shape (n_channels, n_times)\n",
        "    Returns\n",
        "    -------\n",
        "    output : ndarray, shape (n_channels,)\n",
        "    Notes\n",
        "    -----\n",
        "    Alias of the feature function: **kurtosis**\n",
        "    \"\"\"\n",
        "    \n",
        "    ndim = data.ndim\n",
        "    return scipy.stats.kurtosis(data, axis=ndim - 1, fisher=False)\n",
        "def compute_kurtosis(data):\n",
        "    \n",
        "    \"\"\"Kurtosis of the data (per channel).\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : ndarray, shape (n_channels, n_times)\n",
        "    Returns\n",
        "    -------\n",
        "    output : ndarray, shape (n_channels,)\n",
        "    Notes\n",
        "    -----\n",
        "    Alias of the feature function: **kurtosis**\n",
        "    \"\"\"\n",
        "    \n",
        "    ndim = data.ndim\n",
        "    return scipy.stats.kurtosis(data, axis=ndim - 1, fisher=False)\n",
        "\n",
        "def _app_samp_entropy_helper(data, emb, metric='chebyshev',\n",
        "                             approximate=True):\n",
        "    \"\"\"Utility function for `compute_app_entropy`` and `compute_samp_entropy`.\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : ndarray, shape (n_channels, n_times)\n",
        "    emb : int (default: 2)\n",
        "        Embedding dimension.\n",
        "    metric : str (default: chebyshev)\n",
        "        Name of the metric function used with KDTree. The list of available\n",
        "        metric functions is given by: ``KDTree.valid_metrics``.\n",
        "    approximate : bool (default: True)\n",
        "        If True, the returned values will be used to compute the\n",
        "        Approximate Entropy (AppEn). Otherwise, the values are used to compute\n",
        "        the Sample Entropy (SampEn).\n",
        "    Returns\n",
        "    -------\n",
        "    output : ndarray, shape (n_channels, 2)\n",
        "    \"\"\"\n",
        "    _all_metrics = KDTree.valid_metrics\n",
        "    if metric not in _all_metrics:\n",
        "        raise ValueError('The given metric (%s) is not valid. The valid '\n",
        "                         'metric names are: %s' % (metric, _all_metrics))\n",
        "    n_channels, n_times = data.shape\n",
        "    phi = np.empty((n_channels, 2))\n",
        "    for j in range(n_channels):\n",
        "        r = 0.2 * np.std(data[j, :], axis=-1, ddof=1)\n",
        "        # compute phi(emb, r)\n",
        "        _emb_data1 = _embed(data[j, None], emb, 1)[0, :, :]\n",
        "        if approximate:\n",
        "            emb_data1 = _emb_data1\n",
        "        else:\n",
        "            emb_data1 = _emb_data1[:-1, :]\n",
        "        count1 = KDTree(emb_data1, metric=metric).query_radius(\n",
        "            emb_data1, r, count_only=True).astype(np.float64)\n",
        "        # compute phi(emb + 1, r)\n",
        "        emb_data2 = _embed(data[j, None], emb + 1, 1)[0, :, :]\n",
        "        count2 = KDTree(emb_data2, metric=metric).query_radius(\n",
        "            emb_data2, r, count_only=True).astype(np.float64)\n",
        "        if approximate:\n",
        "            phi[j, 0] = np.mean(np.log(count1 / emb_data1.shape[0]))\n",
        "            phi[j, 1] = np.mean(np.log(count2 / emb_data2.shape[0]))\n",
        "        else:\n",
        "            phi[j, 0] = np.mean((count1 - 1) / (emb_data1.shape[0] - 1))\n",
        "            phi[j, 1] = np.mean((count2 - 1) / (emb_data2.shape[0] - 1))\n",
        "    return phi\n",
        "\n",
        "\n",
        "def compute_app_entropy(data, emb=2, metric='chebyshev'):\n",
        "    \"\"\"Approximate Entropy (AppEn, per channel).\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : ndarray, shape (n_channels, n_times)\n",
        "    emb : int (default: 2)\n",
        "        Embedding dimension.\n",
        "    metric : str (default: chebyshev)\n",
        "        Name of the metric function used with\n",
        "        :class:`~sklearn.neighbors.KDTree`. The list of available\n",
        "        metric functions is given by: ``KDTree.valid_metrics``.\n",
        "    Returns\n",
        "    -------\n",
        "    output : ndarray, shape (n_channels,)\n",
        "    Notes\n",
        "    -----\n",
        "    Alias of the feature function: **app_entropy**. See [1]_.\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Richman, J. S. et al. (2000). Physiological time-series analysis\n",
        "           using approximate entropy and sample entropy. American Journal of\n",
        "           Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n",
        "    \"\"\"\n",
        "    phi = _app_samp_entropy_helper(data, emb=emb, metric=metric,\n",
        "                                   approximate=True)\n",
        "    return np.subtract(phi[:, 0], phi[:, 1])\n",
        "\n",
        "\n",
        "def compute_samp_entropy(data, emb=2, metric='chebyshev'):\n",
        "    \n",
        "    \"\"\"Sample Entropy (SampEn, per channel).\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : ndarray, shape (n_channels, n_times)\n",
        "    emb : int (default: 2)\n",
        "        Embedding dimension.\n",
        "    metric : str (default: chebyshev)\n",
        "        Name of the metric function used with KDTree. The list of available\n",
        "        metric functions is given by: `KDTree.valid_metrics`.\n",
        "    Returns\n",
        "    -------\n",
        "    output : ndarray, shape (n_channels,)\n",
        "    Notes\n",
        "    -----\n",
        "    Alias of the feature function: **samp_entropy**. See [1]_.\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Richman, J. S. et al. (2000). Physiological time-series analysis\n",
        "           using approximate entropy and sample entropy. American Journal of\n",
        "           Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n",
        "    \"\"\"\n",
        "    phi = _app_samp_entropy_helper(data, emb=emb, metric=metric,\n",
        "                                   approximate=False)\n",
        "    if np.allclose(phi[:, 0], 0) or np.allclose(phi[:, 1], 0):\n",
        "        raise ValueError('Sample Entropy is not defined.')\n",
        "    else:\n",
        "        return -np.log(np.divide(phi[:, 1], phi[:, 0]))\n",
        "    \n",
        "def _embed(x, d, tau):\n",
        "    \"\"\"Time-delay embedding.\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray, shape (n_channels, n_times)\n",
        "    d : int\n",
        "        Embedding dimension.\n",
        "        The embedding dimension ``d`` should be greater than 2.\n",
        "    tau : int\n",
        "        Delay.\n",
        "        The delay parameter ``tau`` should be less or equal than\n",
        "        ``floor((n_times - 1) / (d - 1))``.\n",
        "    Returns\n",
        "    -------\n",
        "    output : ndarray, shape (n_channels, n_times - (d - 1) * tau, d)\n",
        "    \"\"\"\n",
        "    tau_max = np.floor((x.shape[1] - 1) / (d - 1))\n",
        "    if tau > tau_max:\n",
        "        warn('The given value (%s) for the parameter `tau` exceeds '\n",
        "             '`tau_max = floor((n_times - 1) / (d - 1))`. Using `tau_max` '\n",
        "             'instead.' % tau)\n",
        "        _tau = tau_max\n",
        "    else:\n",
        "        _tau = int(tau)\n",
        "    x = x.copy()\n",
        "    X = np.lib.stride_tricks.as_strided(\n",
        "        x, (x.shape[0], x.shape[1] - d * _tau + _tau, d),\n",
        "        (x.strides[-2], x.strides[-1], x.strides[-1] * _tau))\n",
        "    return X\n",
        "def _app_samp_entropy_helper(data, emb, metric='chebyshev',\n",
        "                             approximate=True):\n",
        "    \"\"\"Utility function for `compute_app_entropy`` and `compute_samp_entropy`.\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : ndarray, shape (n_channels, n_times)\n",
        "    emb : int (default: 2)\n",
        "        Embedding dimension.\n",
        "    metric : str (default: chebyshev)\n",
        "        Name of the metric function used with KDTree. The list of available\n",
        "        metric functions is given by: ``KDTree.valid_metrics``.\n",
        "    approximate : bool (default: True)\n",
        "        If True, the returned values will be used to compute the\n",
        "        Approximate Entropy (AppEn). Otherwise, the values are used to compute\n",
        "        the Sample Entropy (SampEn).\n",
        "    Returns\n",
        "    -------\n",
        "    output : ndarray, shape (n_channels, 2)\n",
        "    \"\"\"\n",
        "    _all_metrics = KDTree.valid_metrics\n",
        "    if metric not in _all_metrics:\n",
        "        raise ValueError('The given metric (%s) is not valid. The valid '\n",
        "                         'metric names are: %s' % (metric, _all_metrics))\n",
        "    n_channels, n_times = data.shape\n",
        "    phi = np.empty((n_channels, 2))\n",
        "    for j in range(n_channels):\n",
        "        r = 0.2 * np.std(data[j, :], axis=-1, ddof=1)\n",
        "        # compute phi(emb, r)\n",
        "        _emb_data1 = _embed(data[j, None], emb, 1)[0, :, :]\n",
        "        if approximate:\n",
        "            emb_data1 = _emb_data1\n",
        "        else:\n",
        "            emb_data1 = _emb_data1[:-1, :]\n",
        "        count1 = KDTree(emb_data1, metric=metric).query_radius(\n",
        "            emb_data1, r, count_only=True).astype(np.float64)\n",
        "        # compute phi(emb + 1, r)\n",
        "        emb_data2 = _embed(data[j, None], emb + 1, 1)[0, :, :]\n",
        "        count2 = KDTree(emb_data2, metric=metric).query_radius(\n",
        "            emb_data2, r, count_only=True).astype(np.float64)\n",
        "        if approximate:\n",
        "            phi[j, 0] = np.mean(np.log(count1 / emb_data1.shape[0]))\n",
        "            phi[j, 1] = np.mean(np.log(count2 / emb_data2.shape[0]))\n",
        "        else:\n",
        "            phi[j, 0] = np.mean((count1 - 1) / (emb_data1.shape[0] - 1))\n",
        "            phi[j, 1] = np.mean((count2 - 1) / (emb_data2.shape[0] - 1))\n",
        "    return phi\n",
        "\n",
        "\n",
        "def compute_app_entropy(data, emb=2, metric='chebyshev'):\n",
        "    \"\"\"Approximate Entropy (AppEn, per channel).\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : ndarray, shape (n_channels, n_times)\n",
        "    emb : int (default: 2)\n",
        "        Embedding dimension.\n",
        "    metric : str (default: chebyshev)\n",
        "        Name of the metric function used with\n",
        "        :class:`~sklearn.neighbors.KDTree`. The list of available\n",
        "        metric functions is given by: ``KDTree.valid_metrics``.\n",
        "    Returns\n",
        "    -------\n",
        "    output : ndarray, shape (n_channels,)\n",
        "    Notes\n",
        "    -----\n",
        "    Alias of the feature function: **app_entropy**. See [1]_.\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Richman, J. S. et al. (2000). Physiological time-series analysis\n",
        "           using approximate entropy and sample entropy. American Journal of\n",
        "           Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n",
        "    \"\"\"\n",
        "    phi = _app_samp_entropy_helper(data, emb=emb, metric=metric,\n",
        "                                   approximate=True)\n",
        "    return np.subtract(phi[:, 0], phi[:, 1])\n",
        "\n",
        "\n",
        "def compute_samp_entropy(data, emb=2, metric='chebyshev'):\n",
        "    \n",
        "    \"\"\"Sample Entropy (SampEn, per channel).\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : ndarray, shape (n_channels, n_times)\n",
        "    emb : int (default: 2)\n",
        "        Embedding dimension.\n",
        "    metric : str (default: chebyshev)\n",
        "        Name of the metric function used with KDTree. The list of available\n",
        "        metric functions is given by: `KDTree.valid_metrics`.\n",
        "    Returns\n",
        "    -------\n",
        "    output : ndarray, shape (n_channels,)\n",
        "    Notes\n",
        "    -----\n",
        "    Alias of the feature function: **samp_entropy**. See [1]_.\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Richman, J. S. et al. (2000). Physiological time-series analysis\n",
        "           using approximate entropy and sample entropy. American Journal of\n",
        "           Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n",
        "    \"\"\"\n",
        "    phi = _app_samp_entropy_helper(data, emb=emb, metric=metric,\n",
        "                                   approximate=False)\n",
        "    if np.allclose(phi[:, 0], 0) or np.allclose(phi[:, 1], 0):\n",
        "        raise ValueError('Sample Entropy is not defined.')\n",
        "    else:\n",
        "        return -np.log(np.divide(phi[:, 1], phi[:, 0]))\n",
        "    \n",
        "def _embed(x, d, tau):\n",
        "    \"\"\"Time-delay embedding.\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : ndarray, shape (n_channels, n_times)\n",
        "    d : int\n",
        "        Embedding dimension.\n",
        "        The embedding dimension ``d`` should be greater than 2.\n",
        "    tau : int\n",
        "        Delay.\n",
        "        The delay parameter ``tau`` should be less or equal than\n",
        "        ``floor((n_times - 1) / (d - 1))``.\n",
        "    Returns\n",
        "    -------\n",
        "    output : ndarray, shape (n_channels, n_times - (d - 1) * tau, d)\n",
        "    \"\"\"\n",
        "    tau_max = np.floor((x.shape[1] - 1) / (d - 1))\n",
        "    if tau > tau_max:\n",
        "        warn('The given value (%s) for the parameter `tau` exceeds '\n",
        "             '`tau_max = floor((n_times - 1) / (d - 1))`. Using `tau_max` '\n",
        "             'instead.' % tau)\n",
        "        _tau = tau_max\n",
        "    else:\n",
        "        _tau = int(tau)\n",
        "    x = x.copy()\n",
        "    X = np.lib.stride_tricks.as_strided(\n",
        "        x, (x.shape[0], x.shape[1] - d * _tau + _tau, d),\n",
        "        (x.strides[-2], x.strides[-1], x.strides[-1] * _tau))\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZnWm91TpfGJ"
      },
      "source": [
        "# Method for finding threshold, 'Thresholding selection rule'\n",
        "def sqtwolog_threshold(wavelet_coeffs):\n",
        "    \"\"\"\n",
        "    Universal thresholding\n",
        "    \"\"\"\n",
        "    denom = scipy.stats.norm.ppf(0.75) # the constant value for Gaussian noise (0.6745)\n",
        "    var = np.median(np.abs(wavelet_coeffs)) / denom\n",
        "    N = len(wavelet_coeffs)\n",
        "    thre = np.sqrt(var) * np.sqrt(2 * np.log(N))\n",
        "    return thre\n",
        "\n",
        "def rigsure_threshold(wavelet_coeffs):\n",
        "    var = np.std(wavelet_coeffs)\n",
        "    N = len(wavelet_coeffs)\n",
        "    sqr_coeffs = []\n",
        "    for coeff in wavelet_coeffs:\n",
        "        sqr_coeffs.append(np.power(coeff, 2))\n",
        "    sqr_coeffs.sort()\n",
        "    pos = 0\n",
        "    r = 0\n",
        "    for idx, sqr_coeff in enumerate(sqr_coeffs):\n",
        "        new_r = (N - 2 * (idx + 1) + (N - (idx + 1))*sqr_coeff + sum(sqr_coeffs[0:idx+1])) / N\n",
        "        if r == 0 or r > new_r:\n",
        "            r = new_r\n",
        "            pos = idx\n",
        "    thre = np.sqrt(var) * np.sqrt(sqr_coeffs[pos])\n",
        "    return thre\n",
        "\n",
        "def heursure_threshold(wavelet_coeffs):\n",
        "    N = len(wavelet_coeffs)\n",
        "    s = 0\n",
        "    for coeff in wavelet_coeffs:\n",
        "        s += np.power(coeff, 2)\n",
        "    theta = (s - N) / N\n",
        "    # It is divide by ...\n",
        "    miu = np.power(np.log2(N), 3/2) / np.power(N, 1/2)\n",
        "    if theta < miu:\n",
        "        return sqtwolog_threshold(wavelet_coeffs)\n",
        "    else:\n",
        "        return min(sqtwolog_threshold(wavelet_coeffs), rigsure_threshold(wavelet_coeffs))\n",
        "    \n",
        "def statistical_threshold(wavelet_coeffs):\n",
        "    \"\"\"\n",
        "    Put wavelet coefficients of one level to the function\n",
        "    Return the statistical threshold value of that level\n",
        "    \"\"\"\n",
        "    threshold_value = 1.5 * np.std(wavelet_coeffs)\n",
        "    return threshold_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um6LEe3XRTGx"
      },
      "source": [
        "def denoised_with_DiscreteWavelet(oneCH_EEG, \n",
        "                                  motherWavelet=\"bior3.9\", decLevel=8, \n",
        "                                  thresholdSelectionRule=\"statistical_threshold\", thresholdingFunction=\"hard\",\n",
        "                                  lcoeffLevel=1, rcoeffLevel=5):\n",
        "    \"\"\"\n",
        "    contEEG is a dataset containing contaminated EEG (54 subjects each has 19 EEG channels)\n",
        "    motherWavelet = 'bior3.9' (Paper)\n",
        "    decLevel = 8 (Calculate)\n",
        "    thresholdSelectionRule = \"heursure_threshold\" or \"statistical_threshold\"\n",
        "    thresholdingFunction = \"soft\" or \"hard\" thresholding\n",
        "    lcoeffLevel = the coefficients corresponding to lower frequency bands that we want to threshold/denoise\n",
        "    rcoeffLevel = the coefficients corresponding to upper frequency bands that we want to threshold/denoise\n",
        "    \"\"\"\n",
        "    \n",
        "    denoised_coeffs = [] # Prepare for contain denoise coefficients\n",
        "    coeffs = pywt.wavedec(oneCH_EEG, wavelet=motherWavelet, level=decLevel)\n",
        "    for ix, coeff in enumerate(coeffs):\n",
        "        # thresholding has been done over the cD from level 8 up to level 3 (Cover OAs freq. band/related coefficients)\n",
        "        threshold_value = None\n",
        "        if ix in range(lcoeffLevel,rcoeffLevel):\n",
        "            if thresholdSelectionRule == \"statistical_threshold\":\n",
        "                threshold_value = statistical_threshold(wavelet_coeffs=coeff)\n",
        "            else:\n",
        "                threshold_value = heursure_threshold(wavelet_coeffs=coeff)\n",
        "\n",
        "            # According to the paper\n",
        "            # wavelet coefficient (wc) is removed if np.abs(wc) > threshold value\n",
        "            denoised_coeff = np.where(np.abs(coeff) > threshold_value, 0, coeff)\n",
        "            denoised_coeffs.append(denoised_coeff)\n",
        "\n",
        "        else:\n",
        "            denoised_coeffs.append(coeff)  \n",
        "        \n",
        "    denoised_oneCH_EEG = pywt.waverec(denoised_coeffs, wavelet=motherWavelet)\n",
        "    return denoised_oneCH_EEG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg7lQ730RTDi"
      },
      "source": [
        "def signals_to_sources(ica, data):\n",
        "    \"\"\"Compute sources from data (operates inplace).\"\"\"\n",
        "    _data = data.copy()\n",
        "    _data = ica._pre_whiten(_data)\n",
        "    if ica.pca_mean_ is not None:\n",
        "        _data -= ica.pca_mean_[:, None]\n",
        "\n",
        "    # Apply unmixing\n",
        "    pca_data = np.dot(ica.unmixing_matrix_,\n",
        "                    ica.pca_components_[:ica.n_components_,:])\n",
        "    # Apply PCA\n",
        "    sources = np.dot(pca_data, _data)\n",
        "    return sources\n",
        "\n",
        "def sources_to_signals(ica, sources):\n",
        "    \"\"\"Compute Mixed signals from sources (operates inplace).\"\"\"\n",
        "    _sources = sources.copy()\n",
        "    # Apply mixing (Inverse of product)\n",
        "    inv_pca_data = np.dot(np.linalg.inv(ica.pca_components_)[:,:ica.n_components_],ica.mixing_matrix_)\n",
        "    # Apply invsersed PCA\n",
        "    _data = np.dot(inv_pca_data, _sources)\n",
        "\n",
        "    if ica.pca_mean_ is not None:\n",
        "        _data += ica.pca_mean_[:, None]\n",
        "    _data *= ica.pre_whitener_\n",
        "    return _data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2efKFPPFRS_X"
      },
      "source": [
        "def ICA_on_raw_N170(raw, EOG_ref=False, ECG_ref=False, \n",
        "                    n_component=None):\n",
        "\n",
        "    # Number of each type of channel\n",
        "    n_eeg = raw.get_channel_types().count('eeg')\n",
        "    n_eog = raw.get_channel_types().count('eog')\n",
        "    n_ecg = raw.get_channel_types().count('ecg')\n",
        "\n",
        "    # Remove artifact\n",
        "    ica = ICA(n_components=n_component, max_iter='auto', \n",
        "            method='infomax',fit_params=dict(extended=True),\n",
        "            random_state=32, verbose=False)\n",
        "    ica.fit(raw)\n",
        "    ica.exclude = []\n",
        "    eog_indices = []\n",
        "    ecg_indices = []\n",
        "    # Find which ICs match the EOG pattern\n",
        "    if EOG_ref == True:\n",
        "        eog_indices, eog_scores = ica.find_bads_eog(raw, verbose=False)\n",
        "    # Find which ICs match the ECG pattern\n",
        "    if ECG_ref == True:\n",
        "        ecg_indices, ecg_scores = ica.find_bads_ecg(raw, method='correlation',\n",
        "                                                    threshold='auto')\n",
        "    # Combined the result from finding EOG and ECG ICs\n",
        "    artifact_indices = list(set(eog_indices + ecg_indices))\n",
        "    ica.exclude = artifact_indices\n",
        "    ica.apply(raw)\n",
        "\n",
        "    return raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQt-IqaZRS5v"
      },
      "source": [
        "def wICA_on_raw_N170(raw, EOG_ref=False, ECG_ref=False):  \n",
        "    \"\"\"\n",
        "    If EOG_ref == True:\n",
        "        Detecting EOG artifact IC(s) depend on EOG reference channels\n",
        "    If EOG_ref == True:\n",
        "        Detecting EOG artifact IC(s) depend on EOG reference channels\n",
        "    Else:\n",
        "        Detecting artifact IC(s) by mMSE and Kurtosis\n",
        "    \"\"\"\n",
        "    filt_raw = raw.copy()\n",
        "\n",
        "    n_eeg = filt_raw.get_channel_types().count('eeg')\n",
        "    n_eog = filt_raw.get_channel_types().count('eog')\n",
        "    n_ecg = filt_raw.get_channel_types().count('ecg')\n",
        "\n",
        "    ica = ICA(n_components=n_eeg, method=\"infomax\", \n",
        "            fit_params=dict(extended=True), random_state=32)\n",
        "    ica.fit(filt_raw)\n",
        "  \n",
        "    # Get ICs\n",
        "    cont_ICs = ica.get_sources(filt_raw).get_data()\n",
        "    cont_IC_samples = cont_ICs.shape[-1]\n",
        "    # Use EOG reference channels to find which IC is EOG pattern\n",
        "    if EOG_ref == True:\n",
        "        eog_indices, eog_scores = ica.find_bads_eog(filt_raw, verbose=False)\n",
        "        artifact_ICs_indexs = np.zeros((n_eeg), dtype=bool)\n",
        "        artifact_ICs_indexs[eog_indices] = True\n",
        "\n",
        "    else:\n",
        "        # Calculate mMSE\n",
        "        coarse_graining_ICs = []\n",
        "        for e_IC in cont_ICs:\n",
        "            \n",
        "            coarse_graining_e_IC = []\n",
        "            length_data = e_IC.shape[-1]\n",
        "            mMSE_scale_factor = 20\n",
        "            range_coarse_graining = np.arange(1, int(length_data/mMSE_scale_factor)+1, 1)\n",
        "\n",
        "            for j in range_coarse_graining:\n",
        "                # Python is zero-index\n",
        "                start_point = ((j-1)*mMSE_scale_factor)\n",
        "                end_point = j * mMSE_scale_factor\n",
        "                coase_graining = (1/mMSE_scale_factor) * np.sum(e_IC[start_point:end_point])\n",
        "                coarse_graining_e_IC.append(coase_graining)\n",
        "                \n",
        "            coarse_graining_ICs.append(np.array(coarse_graining_e_IC))\n",
        "\n",
        "        mMSE_ICs = compute_samp_entropy(data=np.array(coarse_graining_ICs))\n",
        "\n",
        "        # Calculate Kurtosis\n",
        "        kurtosis_ICs= []\n",
        "\n",
        "        for e_IC in cont_ICs:\n",
        "            kurtosis_e_IC = compute_kurtosis(data=e_IC)\n",
        "            kurtosis_ICs.append(kurtosis_e_IC)\n",
        "\n",
        "        kurtosis_ICs = np.array(kurtosis_ICs)\n",
        "\n",
        "        # Check which ICs are artifactual ICs\n",
        "        # alpha level of two tailed t-test = 0.05\n",
        "        alpha = 0.05\n",
        "        critical_value = scipy.stats.t.ppf(q=1-alpha/2,df=int(n_eeg)-1)\n",
        "\n",
        "        # Calculate Upper limit and Lower limit\n",
        "        lower_limit= np.mean(mMSE_ICs) - ((np.std(mMSE_ICs) / np.sqrt(len(mMSE_ICs)) * critical_value))\n",
        "        upper_limit= np.mean(kurtosis_ICs) - ((np.std(kurtosis_ICs) / np.sqrt(len(kurtosis_ICs))) * critical_value)\n",
        "\n",
        "        # print(f\"Check which IC is artifact IC\\n {(mMSE_ICs < lower_limit) & (kurtosis_ICs > upper_limit)}\")\n",
        "        artifact_ICs_indexs = (mMSE_ICs < lower_limit) & (kurtosis_ICs > upper_limit)\n",
        "        non_artifact_ICs_indexs = np.invert(artifact_ICs_indexs)\n",
        "\n",
        "        artifact_ICs = cont_ICs[artifact_ICs_indexs, :]\n",
        "        non_artifact_ICs = cont_ICs[non_artifact_ICs_indexs, :]\n",
        "\n",
        "\n",
        "    # Denoise artifactual ICs with Wavelet\n",
        "    denoised_ICs = []\n",
        "    for idx_IC, IC_bool in zip(range(n_eeg), artifact_ICs_indexs):\n",
        "        \n",
        "        selected_IC = cont_ICs[idx_IC, :]\n",
        "        \n",
        "        # True if IC is artifactual IC\n",
        "        if IC_bool == True:\n",
        "            \n",
        "            denoised_IC = denoised_with_DiscreteWavelet(selected_IC, \n",
        "                                            motherWavelet=\"bior3.9\", decLevel=8, \n",
        "                                            thresholdSelectionRule=\"statistical_threshold\", \n",
        "                                            thresholdingFunction=\"hard\",\n",
        "                                            lcoeffLevel=1, rcoeffLevel=5)\n",
        "            denoised_ICs.append(denoised_IC[:cont_IC_samples])\n",
        "        # False if IC is non-artifactual IC\n",
        "        elif IC_bool == False:\n",
        "            denoised_ICs.append(selected_IC[:cont_IC_samples])\n",
        "\n",
        "    # List to Numpy\n",
        "    denoised_ICs = np.array(denoised_ICs)\n",
        "\n",
        "    # Use ECG reference channels to find which IC is ECG pattern\n",
        "    if ECG_ref == True: \n",
        "        ecg_indices, ecg_scores = ica.find_bads_ecg(filt_raw, method='correlation',\n",
        "                                                    threshold='auto', verbose=False)\n",
        "        # For checking IC(s) is not EOG IC\n",
        "        eog_indices, eog_scores = ica.find_bads_eog(filt_raw, verbose=False)\n",
        "\n",
        "        for ecg_index in ecg_indices:\n",
        "            if ecg_index not in eog_indices:\n",
        "                # Zero out\n",
        "                denoised_ICs[ecg_index,:] = 0\n",
        "\n",
        "    # Report artifactual ICs\n",
        "    if EOG_ref == True and ECG_ref == True:\n",
        "        # wICA with EOG reference and ICA with ECG reference\n",
        "        eog_indices = eog_indices\n",
        "        ecg_indices = ecg_indices\n",
        "        print(f'artifact ICs are {np.union1d(eog_indices, ecg_indices)}')\n",
        "\n",
        "    elif EOG_ref == True and ECG_ref == False:\n",
        "        # wICA with EOG referece\n",
        "        eog_indices = eog_indices\n",
        "        ecg_indices = np.array([])\n",
        "        print(f'artifact ICs are {np.union1d(eog_indices, ecg_indices)}')\n",
        "\n",
        "    elif EOG_ref == False and ECG_ref == True:\n",
        "        # wICA without EOG reference and ICA with ECG reference\n",
        "        eog_indices = np.argwhere(artifact_ICs_indexs == True)\n",
        "        ecg_indices = ecg_indices\n",
        "        print(f'artifact ICs are {np.union1d(eog_indices, ecg_indices)}')\n",
        "        \n",
        "    else: # EOG_ref == False and ECG_ref == False:\n",
        "        eog_indices = np.argwhere(artifact_ICs_indexs == True)\n",
        "        ecg_indices = np.array([])\n",
        "        print(f'artifact ICs are {np.union1d(eog_indices, ecg_indices)}')\n",
        "\n",
        "    # Mix ICs to obtain mixed signals\n",
        "    denoised_EEG = sources_to_signals(ica=ica, sources=denoised_ICs)\n",
        "    # HEOG,VEOG,ECG\n",
        "    non_eeg = raw.copy().pick_types(eeg=False, eog=True, ecg=True, stim=True).get_data()\n",
        "    # Concatenate denoised EEG and non-eeg\n",
        "    denoised_EEG = np.concatenate((denoised_EEG, non_eeg))\n",
        "    # Create raw object for denoised EEG\n",
        "    denoised_raw = raw.copy()\n",
        "    denoised_raw._data = denoised_EEG\n",
        "\n",
        "    return denoised_raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpceYjXfwv8S"
      },
      "source": [
        "def artifact_removal(raw, EOG_ref, ECG_ref):\n",
        "    \"\"\"\n",
        "    Calling a function of artifact removal technique according to \n",
        "    EOG_ref and ECG_ref parameters\n",
        "\n",
        "    EOG_ref has 4 options\n",
        "    1) None\n",
        "    2) 'wICA_without_ref'\n",
        "    3) 'wICA_with_ref'\n",
        "    4) 'EEGANet'\n",
        "\n",
        "    ECG_ref has 3 options\n",
        "    1) None\n",
        "    2) ICA_with_ref\n",
        "    3) wICA_with_ref\n",
        "    \"\"\"\n",
        "    input_raw = raw.copy()\n",
        "    sfreq_after_epoch = 200 # Resample or not\n",
        "\n",
        "    if EOG_ref == None and ECG_ref == None:\n",
        "        output_raw = input_raw\n",
        "\n",
        "    elif EOG_ref == 'wICA_without_ref' and ECG_ref == None:\n",
        "        output_raw = wICA_on_raw_N170(raw, EOG_ref=False, ECG_ref=False)\n",
        "\n",
        "    elif EOG_ref == 'wICA_with_ref' and ECG_ref == None:\n",
        "        output_raw = wICA_on_raw_N170(raw, EOG_ref=True, ECG_ref=False)\n",
        "\n",
        "    elif EOG_ref == None and ECG_ref == 'ICA_with_ref':\n",
        "        output_raw = ICA_on_raw_N170(raw, ECG_ref=True)\n",
        "\n",
        "    elif EOG_ref == 'wICA_without_ref' and ECG_ref == 'ICA_with_ref':\n",
        "        output_raw = wICA_on_raw_N170(raw, EOG_ref=False, ECG_ref=False)\n",
        "\n",
        "    elif EOG_ref == 'wICA_with_ref' and ECG_ref == 'ICA_with_ref':\n",
        "        output_raw = wICA_on_raw_N170(raw, EOG_ref=True, ECG_ref=False)\n",
        "\n",
        "    elif EOG_ref == 'wICA_without_ref' and ECG_ref == 'wICA_with_ref':\n",
        "        output_raw = wICA_on_raw_N170(raw, EOG_ref=False, ECG_ref=False)\n",
        "\n",
        "    elif EOG_ref == 'wICA_with_ref' and ECG_ref == 'wICA_with_ref':\n",
        "        output_raw = wICA_on_raw_N170(raw, EOG_ref=True, ECG_ref=False)\n",
        "\n",
        "    return output_raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIoAnth4qGKy"
      },
      "source": [
        "## Check the folder's name using for containing pre-processed files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syWbGRvHp-2R",
        "outputId": "39e2621f-bbba-4ea4-98c7-e5dee37a1f58"
      },
      "source": [
        "if EOG_ref == None and ECG_ref == None:\n",
        "    ARTIFACT_SUB_FOLDER = f\"raw_{sfreq_after_epoch}Hz-39EEGchs-{EOG_ref}-{ECG_ref}\"\n",
        "\n",
        "elif EOG_ref == 'wICA_without_ref' and ECG_ref == None:\n",
        "    ARTIFACT_SUB_FOLDER = f\"raw_{sfreq_after_epoch}Hz-39EEGchs-{EOG_ref}-{ECG_ref}\"\n",
        "\n",
        "elif EOG_ref == 'wICA_with_ref' and ECG_ref == None:\n",
        "    ARTIFACT_SUB_FOLDER = f\"raw_{sfreq_after_epoch}Hz-39EEGchs-{EOG_ref}-{ECG_ref}\"\n",
        "\n",
        "elif EOG_ref == 'EEGANet' and ECG_ref == None:\n",
        "    ARTIFACT_SUB_FOLDER = f\"raw_{sfreq_after_epoch}Hz-39EEGchs-{EOG_ref}-{ECG_ref}\"\n",
        "\n",
        "elif EOG_ref == None and ECG_ref == 'ICA_with_ref':\n",
        "    ARTIFACT_SUB_FOLDER = f\"raw_{sfreq_after_epoch}Hz-39EEGchs-{EOG_ref}-{ECG_ref}\"\n",
        "\n",
        "elif EOG_ref == None and ECG_ref == 'wICA_with_ref':\n",
        "    ARTIFACT_SUB_FOLDER = f\"raw_{sfreq_after_epoch}Hz-39EEGchs-{EOG_ref}-{ECG_ref}\"\n",
        "\n",
        "elif EOG_ref == 'wICA_without_ref' and ECG_ref == 'ICA_with_ref':\n",
        "    ARTIFACT_SUB_FOLDER = f\"raw_{sfreq_after_epoch}Hz-39EEGchs-{EOG_ref}-{ECG_ref}\"\n",
        "\n",
        "elif EOG_ref == 'wICA_with_ref' and ECG_ref == 'ICA_with_ref':\n",
        "    ARTIFACT_SUB_FOLDER = f\"raw_{sfreq_after_epoch}Hz-39EEGchs-{EOG_ref}-{ECG_ref}\"\n",
        "\n",
        "elif EOG_ref == 'EEGANet' and ECG_ref == 'ICA_with_ref':\n",
        "    ARTIFACT_SUB_FOLDER = f\"raw_{sfreq_after_epoch}Hz-39EEGchs-{EOG_ref}-{ECG_ref}\"\n",
        "\n",
        "elif EOG_ref == 'wICA_without_ref' and ECG_ref == 'wICA_with_ref':\n",
        "    ARTIFACT_SUB_FOLDER = f\"raw_{sfreq_after_epoch}Hz-39EEGchs-{EOG_ref}-{ECG_ref}\"\n",
        "\n",
        "elif EOG_ref == 'wICA_with_ref' and ECG_ref == 'wICA_with_ref':\n",
        "    ARTIFACT_SUB_FOLDER = f\"raw_{sfreq_after_epoch}Hz-39EEGchs-{EOG_ref}-{ECG_ref}\"\n",
        "\n",
        "elif EOG_ref == 'EEGANet' and ECG_ref == 'wICA_with_ref':\n",
        "    ARTIFACT_SUB_FOLDER = f\"raw_{sfreq_after_epoch}Hz-39EEGchs-{EOG_ref}-{ECG_ref}\"\n",
        "\n",
        "print(f\"Folder name is {ARTIFACT_SUB_FOLDER}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder name is raw_200Hz-39EEGchs-wICA_with_ref-ICA_with_ref\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pLWaUjGqR3T"
      },
      "source": [
        "# Raw\n",
        "\n",
        "### Decomposition by DWT\n",
        "\n",
        "From the input signal's sampling frequency was **200 Hz** and OAs occur due to eye moment and eye-blinks and have freuqncy ranges of 0-7 Hz and 8-13 Hz\n",
        "\n",
        "- level 01: 50-100 Hz\n",
        "- level 02: 25-50 Hz\n",
        "- level 03: 12.5-25 Hz\n",
        "- level 04: 6.25-12.5 Hz\n",
        "- level 05: 3.125-6.25 Hz\n",
        "- level 06: 1.5625-3.125 Hz\n",
        "- level 07: 0.78125-1.5625 Hz\n",
        "- level 08: 0.390625-0.78125 Hz\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dhbmf-p4gPXo"
      },
      "source": [
        "for subID in range(1,11):\n",
        "    path = f'Datasets/N170/sub-{subID:02}/ses-meg/meg'\n",
        "    for runID in range(1,7):\n",
        "        # Path of signal and event files\n",
        "        file_fif = f'{path}/sub-{subID:02}_ses-meg_task-facerecognition_run-{runID:02}_meg.fif'\n",
        "\n",
        "        raw = mne.io.read_raw_fif(file_fif,preload=True, verbose=False)\n",
        "        raw.set_channel_types({'EEG061':'eog','EEG062':'eog','EEG063':'ecg','EEG064':'misc'})\n",
        "        # raw.plot(block=True)\n",
        "        raw.rename_channels({'EEG061': 'HEOG',\n",
        "                             'EEG062': 'VEOG',\n",
        "                             'EEG063': 'ECG'})\n",
        "        # Change some channels'types\n",
        "        raw.set_channel_types({'HEOG': 'eog',\n",
        "                               'VEOG': 'eog',\n",
        "                               'ECG': 'ecg',\n",
        "                               'EEG064':'misc'})\n",
        "        # Rename channels\n",
        "        raw.rename_channels(ch_name_dic)\n",
        "        # Pick channels \n",
        "        selected_ch_names = ['Fpz', 'AF3', 'AF4', 'F7', 'F3', 'Fz', 'F4', 'F8',\n",
        "                            'FT9', 'FC5', 'FC1', 'FC2', 'FC6', 'FT10',\n",
        "                            'T9', 'C5', 'C1', 'C2', 'C6', 'T10',\n",
        "                            'TP9', 'CP5', 'CP1', 'CP2', 'CP6', 'TP10',\n",
        "                            'P9', 'P5', 'P1', 'P2', 'P6', 'P10',\n",
        "                            'PO9', 'PO3', 'PO4', 'PO10',\n",
        "                            'O1', 'O2', 'Iz',\n",
        "                            'HEOG', 'VEOG', 'ECG', 'STI101']\n",
        "        raw.pick_channels(selected_ch_names)\n",
        "        \n",
        "        # Read a event file\n",
        "        # 3 columns (event time in samples, stim's value before event, event id)\n",
        "        all_events = mne.find_events(raw,stim_channel='STI101',shortest_event=1,\n",
        "                                    verbose=False) \n",
        "\n",
        "        # Select only stimulus-related events\n",
        "        events = []\n",
        "        for e_event in all_events:\n",
        "            if e_event[2] in [5,6,7,13,14,15,17,18,19]:\n",
        "                events.append(e_event)\n",
        "        events = np.array(events)\n",
        "  \n",
        "        # Re-label 9 labels to 3 labels\n",
        "        # Famous:1, Unfamiliar:2, Scrambled Face:3\n",
        "        events = mne.merge_events(events,[5,6,7],0)\n",
        "        events = mne.merge_events(events,[13,14,15],1)\n",
        "        events = mne.merge_events(events,[17,18,19],2)\n",
        "\n",
        "        # Notch filter and Bandpass\n",
        "        # Remove power line\n",
        "        half_sfreq = int(raw.info['sfreq'] / 2)\n",
        "        raw.notch_filter(freqs=np.arange(50,half_sfreq,50))\n",
        "        raw.filter(1,30,n_jobs=2,fir_design='firwin')\n",
        "\n",
        "\n",
        "        # # Apply Artifact removal technique\n",
        "        raw = artifact_removal(raw, EOG_ref=EOG_ref, ECG_ref=ECG_ref)\n",
        "        \n",
        "        # Drop bad channels \n",
        "        # Visual checking on evoked and some channels went bad after applying artifact removal techniques\n",
        "        raw.drop_channels(['O1', 'O2', 'Iz'])\n",
        "\n",
        "        picks = mne.pick_types(raw.info,meg=False,eeg=True,stim=True,eog=True,ecg=True,misc=False)\n",
        "        epochs = mne.Epochs(raw, events, event_id, tmin=-0.2, tmax=0.6, picks=None,\n",
        "                            baseline=(-0.2, 0), reject=None, preload=True, \n",
        "                            verbose=False)\n",
        "        \n",
        "        # epoch_raw.drop_channels(['EEG061','EEG062','EEG063','EEG064'])\n",
        "        epochs.resample(200.00)\n",
        "        # epochs.rename_channels(ch_name_dic)\n",
        "        # epochs.plot(block=True)\n",
        "        # epochs.info['bads'] = ['EEG061','EEG062','EEG063','EEG064']\n",
        "        # epochs.interpolate_bads(reset_bads=True, mode='accurate', verbose=None)\n",
        "\n",
        "        epochs.set_eeg_reference(ref_channels='average', projection=True)\n",
        "        epochs.apply_proj()\n",
        "        # # Reject signals according their amplitudes\n",
        "        # eeg_reject = dict(eeg=200e-6)\n",
        "\n",
        "        # epochs.drop_bad(reject=eeg_reject, flat=None, verbose=None)\n",
        "\n",
        "        # Plot evoked\n",
        "        evoked_non = epochs['Scrambled'].average()\n",
        "        evoked_non.plot(spatial_colors=True, gfp=True, time_unit='s')\n",
        "        evoked_target = epochs['Famous'].average()\n",
        "        evoked_target.plot(spatial_colors=True, gfp=True, time_unit='s')\n",
        "\n",
        "        epochs.save(fname=f'Datasets/N170-preprocessed-39EEGchs/{ARTIFACT_SUB_FOLDER}/sub-{subID:02}-run-{runID:02}-eeg-epo.fif',\n",
        "                    split_size='2GB',\n",
        "                    overwrite=True,\n",
        "                    verbose=False)\n",
        "        \n",
        "        # Misc\n",
        "        print(f\"Saved sub-{subID:02}-run-{runID:02}-eeg-epo.fif file\")\n",
        "        print('-'*150)\n",
        "        \n",
        "    #     break\n",
        "    # break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvnRpoT8IcrD"
      },
      "source": [
        "### Test start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_-82pYWRTbi"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBmyoH5XRTYr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPcBaL2-RSL8"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixRNZpGRRSIo"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkkheVsBRR-t"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cudzpXxULoLC"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjvzUNJ9ypqH"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxj6NzkyIADX"
      },
      "source": [
        "### Test stop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bagoy-38OcDz"
      },
      "source": [
        "# Epoch and Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w4monlZ_EBi"
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from mne.preprocessing import Xdawn\n",
        "from pyriemann.estimation import ERPCovariances\n",
        "from collections import OrderedDict\n",
        "from pyriemann.tangentspace import TangentSpace\n",
        "from pyriemann.classification import MDM\n",
        "from pyriemann.estimation import XdawnCovariances\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from mne.decoding import Vectorizer\n",
        "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\n",
        "from sklearn.pipeline import make_pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRkQbkE0gbj-",
        "outputId": "3bfbbb75-e4dd-4f1f-d09b-999e6b8f80cf"
      },
      "source": [
        "for subID in range(5,11):\n",
        "    path = f'Datasets/N170-preprocessed-39EEGchs/{ARTIFACT_SUB_FOLDER}/'\n",
        "    fif_list = [f'{path}sub-{subID:02}-run-{runID:02}-eeg-epo.fif'\n",
        "                for runID in range(1,7)]\n",
        "    print(f\"path: {path}\")\n",
        "    list_epochs = []\n",
        "    for file_fif in fif_list:\n",
        "        epochs = mne.read_epochs(fname=file_fif, preload=True, verbose=False)\n",
        "        # Select only EEG channels for classification\n",
        "        epochs.pick_types(eeg=True)\n",
        "        list_epochs.append(epochs)\n",
        "\n",
        "    # Extract features and targets\n",
        "    X = epochs.get_data()\n",
        "    y = epochs.events[:, -1]\n",
        "\n",
        "    # # Select only Famous:0 and Scrambled:2 events\n",
        "    # mask_FS = np.isin(element=y, test_elements=[0,2])\n",
        "\n",
        "    # X = X[mask_FS]\n",
        "    # y = y[mask_FS]\n",
        "\n",
        "    print(X.shape, y.shape)\n",
        "\n",
        "    # Combined Epochs\n",
        "    for idx_epochs, epochs in enumerate(list_epochs):\n",
        "        # mne to Numpy\n",
        "        e_X = epochs.copy().pick_types(eeg=True).get_data()\n",
        "        e_y = epochs.events[:, -1]\n",
        "        # print(e_X.shape)\n",
        "        if idx_epochs == 0:\n",
        "            X = e_X\n",
        "            y = e_y\n",
        "        else:\n",
        "            X = np.concatenate((X, e_X), axis=0)\n",
        "            y = np.concatenate((y, e_y), axis=0)\n",
        "\n",
        "    # # Select only Famous:0 and Scrambled:2 events\n",
        "    # mask_FS = np.isin(element=y, test_elements=[0,2])\n",
        "\n",
        "    # X = X[mask_FS]\n",
        "    # y = y[mask_FS]\n",
        "\n",
        "    print(X.shape, y.shape)\n",
        "\n",
        "    lda = LDA(shrinkage='auto', solver='eigen') #Regularized LDA\n",
        "    lr = LogisticRegression(penalty='l1', solver='liblinear',\n",
        "                                        multi_class='auto')\n",
        "\n",
        "    clfs = OrderedDict()\n",
        "\n",
        "    n_components = 3\n",
        "\n",
        "    clfs['ERP + TS + LR']= make_pipeline(ERPCovariances(estimator='oas'), \n",
        "                                        TangentSpace(), \n",
        "                                        LogisticRegression())\n",
        "    # clfs['ERP + TS + MinMax + LR']= make_pipeline(ERPCovariances(estimator='oas'), \n",
        "    #                                             TangentSpace(), \n",
        "    #                                             MinMaxScaler(),\n",
        "    #                                             LogisticRegression())\n",
        "    clfs['ERP + TS + SVC']= make_pipeline(ERPCovariances(estimator='oas'), \n",
        "                                        TangentSpace(), \n",
        "                                        SVC(decision_function_shape='ovr'))\n",
        "    # clfs['ERP + TS + MinMax + SVC']= make_pipeline(ERPCovariances(estimator='oas'), \n",
        "    #                                             TangentSpace(), \n",
        "    #                                             MinMaxScaler(),\n",
        "    #                                             SVC(decision_function_shape='ovr'))\n",
        "    clfs['ERP + MDM'] = make_pipeline(ERPCovariances(estimator='oas'), \n",
        "                                    MDM())\n",
        "    # clfs['Xdawn + RegLDA'] = make_pipeline(XdawnCovariances(n_components, \n",
        "    #                             estimator='oas'), Vectorizer(), LogisticRegression())\n",
        "    # clfs['Xdawn + MDM'] = make_pipeline(XdawnCovariances(n_components,\n",
        "    #                             estimator='oas'), MDM())\n",
        "\n",
        "    # Cross validator\n",
        "    cv = StratifiedKFold(n_splits=15, shuffle=True, random_state=42)\n",
        "\n",
        "    for clf in clfs:\n",
        "        # Do cross-validation\n",
        "        preds = np.empty(len(y))\n",
        "        for train, test in cv.split(X, y):  #Xdawn takes in epoch object\n",
        "            clfs[clf].fit(X[train], y[train])\n",
        "            preds[test] = clfs[clf].predict(X[test])\n",
        "\n",
        "        # Classification report\n",
        "        target_names = ['0', '1', '2']\n",
        "        report = classification_report(y, preds, target_names=target_names)\n",
        "        print(clf)\n",
        "        print(report)\n",
        "        \n",
        "    print('-'*150)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "path: Datasets/N170-preprocessed-39EEGchs/raw_200Hz-39EEGchs-wICA_with_ref-ICA_with_ref/\n",
            "(147, 36, 160) (147,)\n",
            "(883, 36, 160) (883,)\n",
            "ERP + TS + LR\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.45      0.45       294\n",
            "           1       0.44      0.44      0.44       296\n",
            "           2       0.76      0.77      0.77       293\n",
            "\n",
            "    accuracy                           0.55       883\n",
            "   macro avg       0.55      0.55      0.55       883\n",
            "weighted avg       0.55      0.55      0.55       883\n",
            "\n",
            "ERP + TS + SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.43      0.43       294\n",
            "           1       0.43      0.41      0.42       296\n",
            "           2       0.78      0.81      0.80       293\n",
            "\n",
            "    accuracy                           0.55       883\n",
            "   macro avg       0.55      0.55      0.55       883\n",
            "weighted avg       0.55      0.55      0.55       883\n",
            "\n",
            "ERP + MDM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.47      0.42       294\n",
            "           1       0.33      0.35      0.34       296\n",
            "           2       0.71      0.52      0.60       293\n",
            "\n",
            "    accuracy                           0.45       883\n",
            "   macro avg       0.48      0.45      0.46       883\n",
            "weighted avg       0.48      0.45      0.46       883\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "path: Datasets/N170-preprocessed-39EEGchs/raw_200Hz-39EEGchs-wICA_with_ref-ICA_with_ref/\n",
            "(148, 36, 160) (148,)\n",
            "(885, 36, 160) (885,)\n",
            "ERP + TS + LR\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.47      0.47       296\n",
            "           1       0.47      0.46      0.47       295\n",
            "           2       0.73      0.73      0.73       294\n",
            "\n",
            "    accuracy                           0.55       885\n",
            "   macro avg       0.55      0.55      0.55       885\n",
            "weighted avg       0.55      0.55      0.55       885\n",
            "\n",
            "ERP + TS + SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.46      0.46       296\n",
            "           1       0.46      0.47      0.47       295\n",
            "           2       0.73      0.72      0.72       294\n",
            "\n",
            "    accuracy                           0.55       885\n",
            "   macro avg       0.55      0.55      0.55       885\n",
            "weighted avg       0.55      0.55      0.55       885\n",
            "\n",
            "ERP + MDM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.52      0.50       296\n",
            "           1       0.43      0.50      0.46       295\n",
            "           2       0.71      0.53      0.61       294\n",
            "\n",
            "    accuracy                           0.52       885\n",
            "   macro avg       0.54      0.52      0.52       885\n",
            "weighted avg       0.54      0.52      0.52       885\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "path: Datasets/N170-preprocessed-39EEGchs/raw_200Hz-39EEGchs-wICA_with_ref-ICA_with_ref/\n",
            "(147, 36, 160) (147,)\n",
            "(882, 36, 160) (882,)\n",
            "ERP + TS + LR\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.47      0.47       296\n",
            "           1       0.44      0.44      0.44       292\n",
            "           2       0.77      0.77      0.77       294\n",
            "\n",
            "    accuracy                           0.56       882\n",
            "   macro avg       0.56      0.56      0.56       882\n",
            "weighted avg       0.56      0.56      0.56       882\n",
            "\n",
            "ERP + TS + SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.52      0.51       296\n",
            "           1       0.51      0.49      0.50       292\n",
            "           2       0.75      0.78      0.76       294\n",
            "\n",
            "    accuracy                           0.59       882\n",
            "   macro avg       0.59      0.59      0.59       882\n",
            "weighted avg       0.59      0.59      0.59       882\n",
            "\n",
            "ERP + MDM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.51      0.45       296\n",
            "           1       0.39      0.20      0.26       292\n",
            "           2       0.45      0.56      0.50       294\n",
            "\n",
            "    accuracy                           0.42       882\n",
            "   macro avg       0.42      0.42      0.41       882\n",
            "weighted avg       0.42      0.42      0.41       882\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "path: Datasets/N170-preprocessed-39EEGchs/raw_200Hz-39EEGchs-wICA_with_ref-ICA_with_ref/\n",
            "(147, 36, 160) (147,)\n",
            "(889, 36, 160) (889,)\n",
            "ERP + TS + LR\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.45      0.44       296\n",
            "           1       0.45      0.46      0.46       297\n",
            "           2       0.79      0.76      0.78       296\n",
            "\n",
            "    accuracy                           0.56       889\n",
            "   macro avg       0.56      0.56      0.56       889\n",
            "weighted avg       0.56      0.56      0.56       889\n",
            "\n",
            "ERP + TS + SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.50      0.50       296\n",
            "           1       0.48      0.45      0.47       297\n",
            "           2       0.79      0.81      0.80       296\n",
            "\n",
            "    accuracy                           0.59       889\n",
            "   macro avg       0.59      0.59      0.59       889\n",
            "weighted avg       0.59      0.59      0.59       889\n",
            "\n",
            "ERP + MDM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.56      0.49       296\n",
            "           1       0.45      0.37      0.41       297\n",
            "           2       0.79      0.71      0.75       296\n",
            "\n",
            "    accuracy                           0.55       889\n",
            "   macro avg       0.56      0.55      0.55       889\n",
            "weighted avg       0.56      0.55      0.55       889\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "path: Datasets/N170-preprocessed-39EEGchs/raw_200Hz-39EEGchs-wICA_with_ref-ICA_with_ref/\n",
            "(147, 36, 160) (147,)\n",
            "(883, 36, 160) (883,)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERP + TS + LR\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.40      0.39       290\n",
            "           1       0.41      0.43      0.42       296\n",
            "           2       0.77      0.71      0.74       297\n",
            "\n",
            "    accuracy                           0.52       883\n",
            "   macro avg       0.52      0.51      0.52       883\n",
            "weighted avg       0.52      0.52      0.52       883\n",
            "\n",
            "ERP + TS + SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.38      0.40       290\n",
            "           1       0.43      0.51      0.46       296\n",
            "           2       0.79      0.70      0.74       297\n",
            "\n",
            "    accuracy                           0.53       883\n",
            "   macro avg       0.54      0.53      0.54       883\n",
            "weighted avg       0.55      0.53      0.54       883\n",
            "\n",
            "ERP + MDM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.57      0.44       290\n",
            "           1       0.41      0.24      0.30       296\n",
            "           2       0.57      0.47      0.51       297\n",
            "\n",
            "    accuracy                           0.43       883\n",
            "   macro avg       0.44      0.43      0.42       883\n",
            "weighted avg       0.44      0.43      0.42       883\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "path: Datasets/N170-preprocessed-39EEGchs/raw_200Hz-39EEGchs-wICA_with_ref-ICA_with_ref/\n",
            "(148, 36, 160) (148,)\n",
            "(888, 36, 160) (888,)\n",
            "ERP + TS + LR\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.50      0.49       297\n",
            "           1       0.46      0.45      0.45       296\n",
            "           2       0.66      0.66      0.66       295\n",
            "\n",
            "    accuracy                           0.54       888\n",
            "   macro avg       0.54      0.54      0.54       888\n",
            "weighted avg       0.54      0.54      0.54       888\n",
            "\n",
            "ERP + TS + SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.46      0.47       297\n",
            "           1       0.46      0.42      0.44       296\n",
            "           2       0.62      0.69      0.65       295\n",
            "\n",
            "    accuracy                           0.52       888\n",
            "   macro avg       0.52      0.53      0.52       888\n",
            "weighted avg       0.52      0.52      0.52       888\n",
            "\n",
            "ERP + MDM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.43      0.43       297\n",
            "           1       0.44      0.36      0.40       296\n",
            "           2       0.48      0.56      0.51       295\n",
            "\n",
            "    accuracy                           0.45       888\n",
            "   macro avg       0.45      0.45      0.45       888\n",
            "weighted avg       0.45      0.45      0.45       888\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2kgmuzuxPqb"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}